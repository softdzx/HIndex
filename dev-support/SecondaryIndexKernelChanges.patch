Index: pom.xml
===================================================================
--- pom.xml	(revision 1505907)
+++ pom.xml	(working copy)
@@ -955,6 +955,30 @@
               </sources>
             </configuration>
           </execution>
+          <execution>
+			<id>src-index</id>
+			<phase>generate-sources</phase>
+			<goals>
+				<goal>add-source</goal>
+			</goals>
+			<configuration>
+				<sources>
+				<source>${project.basedir}/secondaryindex/src/main/java</source>
+				</sources>
+			</configuration>
+		  </execution>
+		  <execution>
+			<id>test-index</id>
+			<phase>generate-sources</phase>
+			<goals>
+				<goal>add-test-source</goal>
+			</goals>
+			<configuration>
+				<sources>
+					<source>${project.basedir}/secondaryindex/src/test/java</source>
+				</sources>
+			</configuration>
+		  </execution>
         </executions>
       </plugin>
       <plugin>
Index: security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
===================================================================
--- security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java	(revision 1505907)
+++ security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java	(working copy)
@@ -15,6 +15,8 @@
 package org.apache.hadoop.hbase.security.access;
 
 import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
 import java.net.InetAddress;
 import java.util.Arrays;
 import java.util.Collection;
@@ -48,6 +50,7 @@
 import org.apache.hadoop.hbase.coprocessor.MasterObserver;
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
+import org.apache.hadoop.hbase.coprocessor.RegionObserver;
 import org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment;
 import org.apache.hadoop.hbase.coprocessor.RegionServerObserver;
 import org.apache.hadoop.hbase.filter.CompareFilter;
@@ -902,8 +905,15 @@
   public void prePut(final ObserverContext<RegionCoprocessorEnvironment> c,
       final Put put, final WALEdit edit, final boolean writeToWAL)
       throws IOException {
-    requirePermission("put", TablePermission.Action.WRITE, c.getEnvironment(),
-        put.getFamilyMap());
+    HRegion region = c.getEnvironment().getRegion();
+    // TODO later when HBase supports making some tables as system table we can convert the index
+    // table as a system table. Then the core code itself will handle not contacting ACL for
+    // our index table.
+    boolean isIndexTable = region.getTableDesc().getNameAsString().endsWith("_idx");
+    if (!isIndexTable) {
+      // When it is index table no need to do ACL checks
+      requirePermission("put", TablePermission.Action.WRITE, c.getEnvironment(), put.getFamilyMap());
+    }
   }
 
   @Override
@@ -918,8 +928,16 @@
   public void preDelete(final ObserverContext<RegionCoprocessorEnvironment> c,
       final Delete delete, final WALEdit edit, final boolean writeToWAL)
       throws IOException {
-    requirePermission("delete", TablePermission.Action.WRITE, c.getEnvironment(),
+    HRegion region = c.getEnvironment().getRegion();
+    // TODO later when HBase supports making some tables as system table we can convert the index
+    // table as a system table. Then the core code itself will handle not contacting ACL for
+    // our index table.
+    boolean isIndexTable = region.getTableDesc().getNameAsString().endsWith("_idx");
+    if (!isIndexTable) {
+      // When it is index table no need to do ACL checks
+      requirePermission("delete", TablePermission.Action.WRITE, c.getEnvironment(),
         delete.getFamilyMap());
+    }
   }
 
   @Override
@@ -1330,4 +1348,84 @@
       throws IOException {
     requirePermission("stop", Permission.Action.ADMIN);
   }
+
+  @Override
+  public void preCreateTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+  }
+
+  @Override
+  public void postCreateTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+  }
+
+  @Override
+  public void preDeleteTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void postDeleteTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void preModifyTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HTableDescriptor htd) throws IOException {
+  }
+
+  @Override
+  public void postModifyTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HTableDescriptor htd) throws IOException {
+  }
+
+  @Override
+  public void preAddColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor column) throws IOException {
+  }
+
+  @Override
+  public void postAddColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor column) throws IOException {
+  }
+
+  @Override
+  public void preModifyColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor descriptor) throws IOException {
+  }
+
+  @Override
+  public void postModifyColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor descriptor) throws IOException {
+  }
+
+  @Override
+  public void preDeleteColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, byte[] c) throws IOException {
+  }
+
+  @Override
+  public void postDeleteColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, byte[] c) throws IOException {
+  }
+
+  @Override
+  public void preEnableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void postEnableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void preDisableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void postDisableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
 }
Index: security/src/test/java/org/apache/hadoop/hbase/security/access/TestACLWithIndexTable.java
===================================================================
--- security/src/test/java/org/apache/hadoop/hbase/security/access/TestACLWithIndexTable.java	(revision 0)
+++ security/src/test/java/org/apache/hadoop/hbase/security/access/TestACLWithIndexTable.java	(revision 0)
@@ -0,0 +1,894 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.security.access;
+
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.ResultScanner;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
+import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;
+import org.apache.hadoop.hbase.index.ColumnQualifier.ValueType;
+import org.apache.hadoop.hbase.index.Constants;
+import org.apache.hadoop.hbase.index.IndexSpecification;
+import org.apache.hadoop.hbase.index.IndexedHTableDescriptor;
+import org.apache.hadoop.hbase.index.coprocessor.master.IndexMasterObserver;
+import org.apache.hadoop.hbase.index.coprocessor.regionserver.IndexRegionObserver;
+import org.apache.hadoop.hbase.index.coprocessor.wal.IndexWALObserver;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.zookeeper.ZKAssign;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(LargeTests.class)
+public class TestACLWithIndexTable {
+
+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private static Configuration conf = TEST_UTIL.getConfiguration();
+  // user with all permissions
+  private static User SUPERUSER;
+  private static User TEST_USER;
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    SecureTestUtil.enableSecurity(conf);
+    conf.set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY, AccessController.class.getName() + ","
+        + IndexMasterObserver.class.getName());
+    conf.set(CoprocessorHost.REGION_COPROCESSOR_CONF_KEY, AccessController.class.getName() + ","
+        + SecureBulkLoadEndpoint.class.getName() + "," + IndexRegionObserver.class.getName());
+    conf.set(CoprocessorHost.REGIONSERVER_COPROCESSOR_CONF_KEY, AccessController.class.getName());
+    conf.set(CoprocessorHost.WAL_COPROCESSOR_CONF_KEY, IndexWALObserver.class.getName());
+    conf.setInt("hbase.regionserver.lease.period", 10 * 60 * 1000);
+    conf.setInt("hbase.rpc.timeout", 10 * 60 * 1000);
+    conf.setBoolean("hbase.use.secondary.index", true);
+    TEST_UTIL.startMiniCluster(2);
+
+    TEST_UTIL.waitTableAvailable(AccessControlLists.ACL_TABLE_NAME, 5000);
+    // create a set of test users
+    SUPERUSER = User.createUserForTesting(conf, "admin", new String[] { "supergroup" });
+    TEST_USER = User.createUserForTesting(conf, "testUser", new String[0]);
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @Test(timeout = 180000)
+  public void testCreateTable() throws Exception {
+    // initilize access control
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testCreateTable"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testCreateTable"), null, Permission.Action.ADMIN));
+
+    PrivilegedExceptionAction createTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testCreateTable");
+        HColumnDescriptor hcd = new HColumnDescriptor("cf");
+        htd.addFamily(hcd);
+        IndexSpecification iSpec = new IndexSpecification("spec");
+        iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+        htd.addIndex(iSpec);
+        admin.createTable(htd);
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction createIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testCreateTable_idx");
+        HColumnDescriptor hcd = new HColumnDescriptor("cf");
+        htd.addFamily(hcd);
+        IndexSpecification iSpec = new IndexSpecification("spec");
+        iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+        htd.addIndex(iSpec);
+        admin.createTable(htd);
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(createIndexTable);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(createTable);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      SUPERUSER.runAs(createTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception.Super user should be allowed to create tables");
+    }
+
+  }
+
+  @Test(timeout = 180000)
+  public void testDisableEnableTable() throws Exception {
+    // initilize access control
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class,
+          Bytes.toBytes("testDisableEnableTable"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testDisableEnableTable"), null, Permission.Action.ADMIN));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testDisableEnableTable");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    // Creating the operation to be performed by the user
+    PrivilegedExceptionAction disableTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.disableTable(Bytes.toBytes("testDisableEnableTable"));
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction disableIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.disableTable("testDisableEnableTable_idx");
+        return null;
+      }
+    };
+    PrivilegedExceptionAction enableTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.enableTable(Bytes.toBytes("testDisableEnableTable"));
+        return null;
+      }
+    };
+    PrivilegedExceptionAction enableIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.enableTable(Bytes.toBytes("testDisableEnableTable_idx"));
+        return null;
+      }
+    };
+
+    // Execute all operations one by one
+    try {
+      TEST_USER.runAs(disableIndexTable);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+
+    }
+    Assert.assertTrue("Index table should be enabled",
+      admin.isTableEnabled("testDisableEnableTable_idx"));
+
+    try {
+      TEST_USER.runAs(disableTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception");
+    }
+    while (!admin.isTableDisabled("testDisableEnableTable")) {
+      Thread.sleep(10);
+    }
+    while (!admin.isTableDisabled("testDisableEnableTable_idx")) {
+      Thread.sleep(10);
+    }
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+    Assert.assertTrue("Main table should be disabled",
+      admin.isTableDisabled("testDisableEnableTable"));
+    Assert.assertTrue("Index table should be disabled",
+      admin.isTableDisabled("testDisableEnableTable_idx"));
+
+    try {
+      TEST_USER.runAs(enableIndexTable);
+      Assert.fail("Index table should not get enabled. It should throw exception");
+    } catch (Exception e) {
+
+    }
+    Assert.assertTrue("Index table should be disabled",
+      admin.isTableDisabled("testDisableEnableTable_idx"));
+    try {
+      TEST_USER.runAs(enableTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception.");
+    }
+    while (!admin.isTableEnabled("testDisableEnableTable")) {
+      Thread.sleep(10);
+    }
+    while (!admin.isTableEnabled("testDisableEnableTable_idx")) {
+      Thread.sleep(10);
+    }
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    Assert.assertTrue("Main table should be enabled",
+      admin.isTableEnabled("testDisableEnableTable"));
+    Assert.assertTrue("Index table should be enabled",
+      admin.isTableEnabled("testDisableEnableTable_idx"));
+
+  }
+
+  @Test(timeout = 180000)
+  public void testScanOperation() throws Exception {
+    // initilize access control
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testScanOperation"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testScanOperation"), null, Permission.Action.READ));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testScanOperation");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    HTable table = new HTable(conf, "testScanOperation");
+    Put p = null;
+    for (int i = 0; i < 10; i++) {
+      p = new Put(Bytes.toBytes("row" + i));
+      p.add(Bytes.toBytes("cf"), Bytes.toBytes("q"), Bytes.toBytes("Test_val"));
+      table.put(p);
+    }
+
+    PrivilegedExceptionAction scanTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HTable table = new HTable(conf, "testScanOperation");
+        SingleColumnValueFilter svf =
+            new SingleColumnValueFilter(Bytes.toBytes("cf"), Bytes.toBytes("q"), CompareOp.EQUAL,
+                Bytes.toBytes("Test_val"));
+        Scan s = new Scan();
+        s.setFilter(svf);
+        ResultScanner scanner = null;
+        try {
+          scanner = table.getScanner(s);
+          Result result = scanner.next();
+          while (result != null) {
+            result = scanner.next();
+          }
+        } finally {
+          if (scanner != null) {
+            scanner.close();
+          }
+        }
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction scanIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HTable table = new HTable(conf, "testScanOperation_idx");
+        Scan s = new Scan();
+        ResultScanner scanner = null;
+        try {
+          scanner = table.getScanner(s);
+          Result result = scanner.next();
+          while (result != null) {
+            result = scanner.next();
+          }
+        } finally {
+          if (scanner != null) {
+            scanner.close();
+          }
+        }
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(scanIndexTable);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(scanTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception.");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testCheckAndPut() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testCheckAndPut"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testCheckAndPut"), null, Permission.Action.WRITE, Permission.Action.READ));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testCheckAndPut");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    final IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    HTable table = new HTable(conf, "testCheckAndPut");
+    Put p = null;
+    for (int i = 0; i < 10; i++) {
+      p = new Put(Bytes.toBytes("row" + i));
+      p.add(Bytes.toBytes("cf"), Bytes.toBytes("q"), Bytes.toBytes("Test_val"));
+      table.put(p);
+    }
+
+    HTable indexTable = new HTable(conf, "testCheckAndPut_idx");
+    p = new Put(Bytes.toBytes("row" + 0));
+    p.add(Constants.IDX_COL_FAMILY, Constants.IDX_COL_QUAL, Bytes.toBytes("old_val"));
+    indexTable.put(p);
+
+    PrivilegedExceptionAction putInIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HTable indexTable2 = new HTable(conf, "testCheckAndPut_idx");
+        Put put = new Put(Bytes.toBytes("row" + 0));
+        put.add(Constants.IDX_COL_FAMILY, Constants.IDX_COL_QUAL, Bytes.toBytes("latest"));
+        indexTable2.checkAndPut(Bytes.toBytes("row" + 0), Constants.IDX_COL_FAMILY,
+          Constants.IDX_COL_QUAL, Bytes.toBytes("old_val"), put);
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction putInMainTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HTable table2 = new HTable(conf, "testCheckAndPut");
+        Put put = new Put(Bytes.toBytes("row" + 0));
+        put.add(Bytes.toBytes("cf"), Bytes.toBytes("q"), Bytes.toBytes("latest"));
+        table2.checkAndPut(Bytes.toBytes("row" + 0), Bytes.toBytes("cf"), Bytes.toBytes("q"),
+          Bytes.toBytes("Test_val"), put);
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(putInIndexTable);
+      Assert.fail("Should throw exception.");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(putInMainTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception.");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testMoveRegionOp() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testMoveRegionOp"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testMoveRegionOp"), null, Permission.Action.ADMIN));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testMoveRegionOp");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    PrivilegedExceptionAction moveMainTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        List<HRegionInfo> tableRegions = admin.getTableRegions(Bytes.toBytes("testMoveRegionOp"));
+        if (tableRegions.size() > 0) {
+          HRegionInfo regionMoved = tableRegions.get(0);
+          admin.move(regionMoved.getEncodedNameAsBytes(), null);
+        }
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction moveIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        List<HRegionInfo> tableRegions =
+            admin.getTableRegions(Bytes.toBytes("testMoveRegionOp_idx"));
+        if (tableRegions.size() > 0) {
+          HRegionInfo regionMoved = tableRegions.get(0);
+          admin.move(regionMoved.getEncodedNameAsBytes(), null);
+        }
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(moveIndexTable);
+      Assert.fail("Should throw exception.");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(moveMainTable);
+    } catch (Exception e) {
+      Assert.fail("should not throw any exception");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testUnassignOperation() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class,
+          Bytes.toBytes("testUnassignOperation"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testUnassignOperation"), null, Permission.Action.ADMIN));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testUnassignOperation");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    PrivilegedExceptionAction uassignMainRegion = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        List<HRegionInfo> tableRegions =
+            admin.getTableRegions(Bytes.toBytes("testUnassignOperation"));
+        if (tableRegions.size() > 0) {
+          HRegionInfo regionToBeUassigned = tableRegions.get(0);
+          admin.unassign(regionToBeUassigned.getRegionName(), false);
+        }
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction uassignIndexRegion = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        List<HRegionInfo> tableRegions =
+            admin.getTableRegions(Bytes.toBytes("testUnassignOperation_idx"));
+        if (tableRegions.size() > 0) {
+          HRegionInfo regionToBeUassigned = tableRegions.get(0);
+          admin.unassign(regionToBeUassigned.getRegionName(), false);
+        }
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(uassignIndexRegion);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(uassignMainRegion);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testSplitOp() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testSplitOp"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testSplitOp"), null, Permission.Action.ADMIN));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testSplitOp");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    HTable table = new HTable(conf, "testSplitOp");
+    Put p = null;
+    for (int i = 0; i < 10; i++) {
+      p = new Put(Bytes.toBytes("row" + i));
+      p.add(Bytes.toBytes("cf"), Bytes.toBytes("q"), Bytes.toBytes("test_val"));
+      table.put(p);
+    }
+
+    PrivilegedExceptionAction splitIndexRegion = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.split("testSplitOp_idx");
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction splitMainRegion = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.split("testSplitOp");
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(splitIndexRegion);
+      Assert.fail("Should throw exception.");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(splitMainRegion);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testCompactionOp() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testCompactionOp"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testCompactionOp"), null, Permission.Action.ADMIN));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testCompactionOp");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    HTable table = new HTable(conf, "testCompactionOp");
+    Put p = null;
+    for (int i = 0; i < 100; i++) {
+      p = new Put(Bytes.toBytes("row" + i));
+      p.add(Bytes.toBytes("cf"), Bytes.toBytes("q"), Bytes.toBytes("test_val"));
+      table.put(p);
+      if (i % 10 == 0) {
+        admin.flush("testCompactionOp");
+        admin.flush("testCompactionOp_idx");
+      }
+    }
+
+    PrivilegedExceptionAction compactIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.compact("testCompactionOp_idx");
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction compactMainTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.compact("testCompactionOp");
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(compactIndexTable);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+    }
+
+    try {
+      TEST_USER.runAs(compactMainTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testDeleteTable() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testDeleteTable"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testDeleteTable"), null, Permission.Action.ADMIN, Permission.Action.CREATE));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testDeleteTable");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    admin.disableTable("testDeleteTable");
+    while (!admin.isTableDisabled("testDeleteTable")) {
+      Thread.sleep(10);
+    }
+    while (!admin.isTableDisabled("testDeleteTable_idx")) {
+      Thread.sleep(10);
+    }
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    PrivilegedExceptionAction deleteIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin2 = new HBaseAdmin(conf);
+        admin2.deleteTable(Bytes.toBytes("testDeleteTable_idx"));
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction deleteMainTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin2 = new HBaseAdmin(conf);
+        admin2.deleteTable(Bytes.toBytes("testDeleteTable"));
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(deleteIndexTable);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(deleteMainTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testflushOp() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testflushOp"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testflushOp"), null, Permission.Action.ADMIN, Permission.Action.CREATE));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testflushOp");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+
+    HTable table = new HTable(conf, "testflushOp");
+    Put p = null;
+    for (int i = 0; i < 100; i++) {
+      p = new Put(Bytes.toBytes("row" + i));
+      p.add(Bytes.toBytes("cf"), Bytes.toBytes("q"), Bytes.toBytes("test_val"));
+      table.put(p);
+    }
+
+    PrivilegedExceptionAction flushIndexRegion = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.flush("testflushOp_idx");
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction flushMainRegion = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        admin.flush("testflushOp");
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(flushIndexRegion);
+      Assert.fail("Should throw exception.");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(flushMainRegion);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception.");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testModifyTable() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testModifyTable"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testModifyTable"), null, Permission.Action.ADMIN, Permission.Action.CREATE));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testModifyTable");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+    admin.disableTable("testModifyTable");
+
+    PrivilegedExceptionAction modifyIndexTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        HTableDescriptor htd = new HTableDescriptor("testModifyTable_idx");
+        htd.addFamily(new HColumnDescriptor("d"));
+        htd.addFamily(new HColumnDescriptor("d1"));
+        admin.modifyTable(Bytes.toBytes("testModifyTable_idx"), htd);
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction modifyMainTable = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        IndexedHTableDescriptor ihtd = new IndexedHTableDescriptor("testModifyTable");
+        HColumnDescriptor hcd = new HColumnDescriptor("cf");
+        HColumnDescriptor hcd1 = new HColumnDescriptor("cf1");
+        ihtd.addFamily(hcd);
+        ihtd.addFamily(hcd1);
+        IndexSpecification iSpec = new IndexSpecification("spec");
+        IndexSpecification iSpec1 = new IndexSpecification("spec1");
+        iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+        iSpec1.addIndexColumn(hcd1, "q", ValueType.String, 10);
+        ihtd.addIndex(iSpec);
+        ihtd.addIndex(iSpec1);
+        admin.modifyTable(Bytes.toBytes("testModifyTable"), ihtd);
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(modifyIndexTable);
+      Assert.fail("Should throw exception.");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(modifyMainTable);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception");
+    }
+  }
+
+  @Test(timeout = 180000)
+  public void testModifyFamily() throws Exception {
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, Bytes.toBytes("testModifyFamily"));
+    protocol.grant(new UserPermission(Bytes.toBytes(TEST_USER.getShortName()), Bytes
+        .toBytes("testModifyFamily"), null, Permission.Action.ADMIN, Permission.Action.CREATE));
+
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    IndexedHTableDescriptor htd = new IndexedHTableDescriptor("testModifyFamily");
+    HColumnDescriptor hcd = new HColumnDescriptor("cf");
+    htd.addFamily(hcd);
+    IndexSpecification iSpec = new IndexSpecification("spec");
+    iSpec.addIndexColumn(hcd, "q", ValueType.String, 10);
+    htd.addIndex(iSpec);
+    admin.createTable(htd);
+    ZKAssign.blockUntilNoRIT(HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL));
+    admin.disableTable("testModifyFamily");
+
+    PrivilegedExceptionAction modifyIndexFamily = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        HColumnDescriptor hcd = new HColumnDescriptor("d");
+        hcd.setMaxVersions(4);
+        admin.modifyColumn("testModifyFamily_idx", hcd);
+        return null;
+      }
+    };
+
+    PrivilegedExceptionAction modifyMainFamily = new PrivilegedExceptionAction() {
+      @Override
+      public Object run() throws Exception {
+        HBaseAdmin admin = new HBaseAdmin(conf);
+        HColumnDescriptor hcd = new HColumnDescriptor("cf");
+        hcd.setMaxVersions(4);
+        admin.modifyColumn("testModifyFamily", hcd);
+        return null;
+      }
+    };
+
+    try {
+      TEST_USER.runAs(modifyIndexFamily);
+      Assert.fail("Should throw exception");
+    } catch (Exception e) {
+
+    }
+
+    try {
+      TEST_USER.runAs(modifyMainFamily);
+    } catch (Exception e) {
+      Assert.fail("Should not throw any exception");
+    }
+  }
+
+}
Index: src/assembly/all.xml
===================================================================
--- src/assembly/all.xml	(revision 1505907)
+++ src/assembly/all.xml	(working copy)
@@ -49,6 +49,11 @@
       <fileMode>0644</fileMode>
       <directoryMode>0755</directoryMode>
     </fileSet>
+	<fileSet>
+		<directory>secondaryindex</directory>
+		<fileMode>0644</fileMode>
+		<directoryMode>0755</directoryMode>
+	</fileSet>
     <fileSet>
       <directory>conf</directory>
       <fileMode>0644</fileMode>
Index: src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java	(working copy)
@@ -74,11 +74,10 @@
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.Writables;
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.util.StringUtils;
 
-import com.google.protobuf.ServiceException;
-
 /**
  * Provides an interface to manage HBase database table metadata + general
  * administrative functions.  Use HBaseAdmin to create, drop, list, enable and
@@ -101,6 +100,7 @@
   private final int retryLongerMultiplier;
   private boolean aborted;
 
+  static final String INDEX_TABLE_SUFFIX = "_idx";
   private static volatile boolean synchronousBalanceSwitchSupported = true;
 
   /**
@@ -420,69 +420,66 @@
     int numRegs = splitKeys == null ? 1 : splitKeys.length + 1;
     int prevRegCount = 0;
     boolean doneWithMetaScan = false;
-    for (int tries = 0; tries < this.numRetries * this.retryLongerMultiplier;
-      ++tries) {
-      if (!doneWithMetaScan) {
-        // Wait for new table to come on-line
-        final AtomicInteger actualRegCount = new AtomicInteger(0);
-        MetaScannerVisitor visitor = new MetaScannerVisitorBase() {
-          @Override
-          public boolean processRow(Result rowResult) throws IOException {
-            if (rowResult == null || rowResult.size() <= 0) {
-              return true;
+
+    MetaScannerVisitorBaseWithTableName userTableVisitor = null;
+    MetaScannerVisitorBaseWithTableName indexTableVisitor = null;
+    boolean indexedHTD = isIndexedHTD(desc);
+
+    for (int tries = 0; tries < this.numRetries * this.retryLongerMultiplier; ++tries) {
+
+      AtomicInteger actualRegCount = null;
+      // Wait for new table to come on-line
+      if (userTableVisitor == null) {
+        userTableVisitor = new MetaScannerVisitorBaseWithTableName(desc.getNameAsString());
+      }
+      actualRegCount = userTableVisitor.getActualRgnCnt();
+      actualRegCount.set(0);
+      MetaScanner.metaScan(conf, userTableVisitor, desc.getName());
+      if (actualRegCount.get() != numRegs) {
+        if (tries == this.numRetries * this.retryLongerMultiplier - 1) {
+          throw new RegionOfflineException("Only " + actualRegCount.get() + " of " + numRegs
+              + " regions are online; retries exhausted.");
+        }
+        try { // Sleep
+          Thread.sleep(getPauseTime(tries));
+        } catch (InterruptedException e) {
+          throw new InterruptedIOException("Interrupted when opening" + " regions; "
+              + actualRegCount.get() + " of " + numRegs + " regions processed so far");
+        }
+        if (actualRegCount.get() > prevRegCount) { // Making progress
+          prevRegCount = actualRegCount.get();
+          tries = -1;
+        }
+      } else {
+        if (indexedHTD) {
+          String indexTableName = desc.getNameAsString() + INDEX_TABLE_SUFFIX;
+          if (indexTableVisitor == null) {
+            indexTableVisitor = new MetaScannerVisitorBaseWithTableName(indexTableName);
+          }
+          actualRegCount = indexTableVisitor.getActualRgnCnt();
+          actualRegCount.set(0);
+          MetaScanner.metaScan(conf, indexTableVisitor, Bytes.toBytes(indexTableName));
+          if (actualRegCount.get() != numRegs) {
+            if (tries == this.numRetries * this.retryLongerMultiplier - 1) {
+              throw new RegionOfflineException("Only " + actualRegCount.get() + " of " + numRegs
+                  + " regions are online; retries exhausted.");
             }
-            HRegionInfo info = MetaReader.parseHRegionInfoFromCatalogResult(
-              rowResult, HConstants.REGIONINFO_QUALIFIER);
-            if (info == null) {
-              LOG.warn("No serialized HRegionInfo in " + rowResult);
-              return true;
+            try { // Sleep
+              Thread.sleep(getPauseTime(tries));
+            } catch (InterruptedException e) {
+              throw new InterruptedIOException("Interrupted when opening" + " regions; "
+                  + actualRegCount.get() + " of " + numRegs + " regions processed so far");
             }
-            if (!(Bytes.equals(info.getTableName(), desc.getName()))) {
-              return false;
+            if (actualRegCount.get() > prevRegCount) { // Making progress
+              prevRegCount = actualRegCount.get();
+              tries = -1;
             }
-            String hostAndPort = null;
-            byte [] value = rowResult.getValue(HConstants.CATALOG_FAMILY,
-              HConstants.SERVER_QUALIFIER);
-            // Make sure that regions are assigned to server
-            if (value != null && value.length > 0) {
-              hostAndPort = Bytes.toString(value);
-            }
-            if (!(info.isOffline() || info.isSplit()) && hostAndPort != null) {
-              actualRegCount.incrementAndGet();
-            }
-            return true;
+          } else if (isTableEnabled(indexTableName)) {
+            return;
           }
-        };
-        MetaScanner.metaScan(conf, visitor, desc.getName());
-        if (actualRegCount.get() != numRegs) {
-          if (tries == this.numRetries * this.retryLongerMultiplier - 1) {
-            throw new RegionOfflineException("Only " + actualRegCount.get() +
-              " of " + numRegs + " regions are online; retries exhausted.");
-          }
-          try { // Sleep
-            Thread.sleep(getPauseTime(tries));
-          } catch (InterruptedException e) {
-            throw new InterruptedIOException("Interrupted when opening" +
-              " regions; " + actualRegCount.get() + " of " + numRegs +
-              " regions processed so far");
-          }
-          if (actualRegCount.get() > prevRegCount) { // Making progress
-            prevRegCount = actualRegCount.get();
-            tries = -1;
-          }
-        } else {
-          doneWithMetaScan = true;
-          tries = -1;
+        } else if (isTableEnabled(desc.getName())) {
+          return;
         }
-      } else if (isTableEnabled(desc.getName())) {
-        return;
-      } else {
-        try { // Sleep
-          Thread.sleep(getPauseTime(tries));
-        } catch (InterruptedException e) {
-          throw new InterruptedIOException("Interrupted when waiting" +
-            " for table to be enabled; meta scan was done");
-        }
       }
     }
     throw new TableNotEnabledException(
@@ -490,6 +487,58 @@
       + desc.getNameAsString() + " to be enabled");
   }
 
+  private boolean isIndexedHTD(final HTableDescriptor desc) {
+    try {
+      Class<?> IndexedHTDKlass = HTableDescriptor.class;
+      IndexedHTDKlass = Class.forName("org.apache.hadoop.hbase.index.IndexedHTableDescriptor");
+      return IndexedHTDKlass.isInstance(desc);
+    } catch (ClassNotFoundException e) {
+      // Nothing to do.
+    }
+    return false;
+  }
+
+  public class MetaScannerVisitorBaseWithTableName implements MetaScannerVisitor {
+    byte[] tableName = null;
+    AtomicInteger actualRegCount = new AtomicInteger(0);
+
+    MetaScannerVisitorBaseWithTableName(String tableName) {
+      this.tableName = Bytes.toBytes(tableName);
+    }
+
+    AtomicInteger getActualRgnCnt() {
+      return actualRegCount;
+    }
+
+    @Override
+    public void close() throws IOException {
+    }
+
+    @Override
+    public boolean processRow(Result rowResult) throws IOException {
+      HRegionInfo info =
+          Writables.getHRegionInfoOrNull(rowResult.getValue(HConstants.CATALOG_FAMILY,
+            HConstants.REGIONINFO_QUALIFIER));
+      // If regioninfo is null, skip this row
+      if (null == info) {
+        return true;
+      }
+      if (!(Bytes.equals(info.getTableName(), tableName))) {
+        return false;
+      }
+      String hostAndPort = null;
+      byte[] value = rowResult.getValue(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER);
+      // Make sure that regions are assigned to server
+      if (value != null && value.length > 0) {
+        hostAndPort = Bytes.toString(value);
+      }
+      if (!(info.isOffline() || info.isSplit()) && hostAndPort != null) {
+        actualRegCount.incrementAndGet();
+      }
+      return true;
+    }
+  }
+
   /**
    * Creates a new table but does not block and wait for it to come online.
    * Asynchronous operation.  To check if the table exists, use
@@ -937,7 +986,21 @@
     if(!tableExists(tableName)){
       throw new TableNotFoundException(Bytes.toString(tableName));
     }
-    return connection.isTableEnabled(tableName);
+
+    boolean indexEnabled = this.conf.getBoolean("hbase.use.secondary.index", false);
+    if (!indexEnabled) {
+      return connection.isTableEnabled(tableName);
+    } else {
+      boolean isTableEnabled = connection.isTableEnabled(tableName);
+      if (isTableEnabled && !isIndexTable(Bytes.toString(tableName))) {
+        String indexTableName = getIndexTableName(Bytes.toString(tableName));
+        if (connection.isTableAvailable(Bytes.toBytes(indexTableName))) {
+          return connection.isTableEnabled(Bytes.toBytes(indexTableName));
+        }
+        return true;
+      }
+      return isTableEnabled;
+    }
   }
 
   /**
@@ -949,6 +1012,16 @@
     return isTableDisabled(Bytes.toBytes(tableName));
   }
 
+  public static boolean isIndexTable(String tableName) {
+    return tableName.endsWith(INDEX_TABLE_SUFFIX);
+  }
+
+  public static String getIndexTableName(String tableName) {
+    // TODO The suffix for the index table is fixed now. Do we allow to make this configurable?
+    // We can handle things in byte[] way?
+    return tableName + INDEX_TABLE_SUFFIX;
+  }
+
   /**
    * @param tableName name of table to check
    * @return true if table is off-line
@@ -958,7 +1031,20 @@
     if (!HTableDescriptor.isMetaTable(tableName)) {
       HTableDescriptor.isLegalTableName(tableName);
     }
-    return connection.isTableDisabled(tableName);
+    boolean indexEnabled = this.conf.getBoolean("hbase.use.secondary.index", false);
+    if (!indexEnabled) {
+      return connection.isTableDisabled(tableName);
+    } else {
+      boolean isTableDisabled = connection.isTableDisabled(tableName);
+      if (isTableDisabled && !isIndexTable(Bytes.toString(tableName))) {
+        String indexTableName = getIndexTableName(Bytes.toString(tableName));
+        if (connection.isTableAvailable(Bytes.toBytes(indexTableName))) {
+          return connection.isTableDisabled(Bytes.toBytes(indexTableName));
+        }
+        return true;
+      }
+      return isTableDisabled;
+    }
   }
 
   /**
@@ -2315,4 +2401,28 @@
   public void deleteSnapshot(final String snapshotName) throws IOException {
     deleteSnapshot(Bytes.toBytes(snapshotName));
   }
+
+  /**
+   * Forcefully sets the table state as DISABLED in ZK
+   * @param tableName
+   */
+  public void setDisableTable(String tableName) throws IOException {
+    setTableState(tableName, "DISABLED");
+  }
+
+  /**
+   * Forcefully sets the table state as ENABLED in ZK
+   * @param tablename
+   */
+  public void setEnableTable(String tableName) throws IOException {
+    setTableState(tableName, "ENABLED");
+  }
+
+  private void setTableState(String tableName, String state) throws IOException {
+    try {
+      getMaster().setTableState(tableName, state);
+    } catch (RemoteException e) {
+      throw e.unwrapRemoteException();
+    }
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/client/HTable.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HTable.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/client/HTable.java	(working copy)
@@ -29,6 +29,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Collections;
+import java.util.Map.Entry;
 import java.util.NavigableMap;
 import java.util.TreeMap;
 import java.util.concurrent.ExecutorService;
@@ -424,6 +425,22 @@
   public byte[][] getEndKeys() throws IOException {
     return getStartEndKeys().getSecond();
   }
+  
+  /**
+   * Get the split keys of the currently open table.
+   * @return Array of split keys
+   * @throws IOException
+   */
+  public byte[][] getSplitKeys() throws IOException {
+    List<byte[]> splitKeysList = new ArrayList<byte[]>();
+    for (HRegionInfo regionInfo : getRegionLocations().keySet()) {
+      if (Bytes.equals(regionInfo.getStartKey(), HConstants.EMPTY_BYTE_ARRAY)) {
+        continue;
+      }
+      splitKeysList.add(regionInfo.getStartKey());
+    }
+    return splitKeysList.toArray(new byte[splitKeysList.size()][]);
+  }
 
   /**
    * Gets the starting and ending row keys for every region in the currently
@@ -726,14 +743,10 @@
    * {@inheritDoc}
    */
   @Override
-  public void delete(final Delete delete)
-  throws IOException {
-    new ServerCallable<Boolean>(connection, tableName, delete.getRow(), operationTimeout) {
-          public Boolean call() throws IOException {
-            server.delete(location.getRegionInfo().getRegionName(), delete);
-            return null; // FindBugs NP_BOOLEAN_RETURN_NULL
-          }
-        }.withRetries();
+  public void delete(final Delete delete) throws IOException {
+    List<Delete> deletes = new ArrayList<Delete>(1);
+    deletes.add(delete);
+    delete(deletes);
   }
 
   /**
Index: src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.client.HConnectionManager.HConnectable;
+import org.apache.hadoop.hbase.errorhandling.TimeoutException;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Writables;
 
@@ -411,27 +412,35 @@
         HTable metaTable = getMetaTable();
         long start = System.currentTimeMillis();
         if (splitA != null) {
-          Result resultA = getRegionResultBlocking(metaTable, blockingTimeout,
-              splitA.getRegionName());
-          if (resultA != null) {
-            processRow(resultA);
-            daughterRegions.add(splitA.getRegionName());
-          } else {
+          try {
+            Result resultA = getRegionResultBlocking(metaTable, blockingTimeout,
+              info.getRegionName(), splitA.getRegionName());
+            if (resultA != null) {
+              processRow(resultA);
+              daughterRegions.add(splitA.getRegionName());
+            }
+            // else parent is gone, so skip this daughter
+          } catch (TimeoutException e) {
             throw new RegionOfflineException("Split daughter region " +
-                splitA.getRegionNameAsString() + " cannot be found in META.");
+                splitA.getRegionNameAsString() + " cannot be found in META. Parent:" +
+                info.getRegionNameAsString());
           }
         }
         long rem = blockingTimeout - (System.currentTimeMillis() - start);
 
         if (splitB != null) {
-          Result resultB = getRegionResultBlocking(metaTable, rem,
-              splitB.getRegionName());
-          if (resultB != null) {
-            processRow(resultB);
-            daughterRegions.add(splitB.getRegionName());
-          } else {
+          try {
+            Result resultB = getRegionResultBlocking(metaTable, rem,
+              info.getRegionName(), splitB.getRegionName());
+            if (resultB != null) {
+              processRow(resultB);
+              daughterRegions.add(splitB.getRegionName());
+            }
+            // else parent is gone, so skip this daughter
+          } catch (TimeoutException e) {
             throw new RegionOfflineException("Split daughter region " +
-                splitB.getRegionNameAsString() + " cannot be found in META.");
+                splitB.getRegionNameAsString() + " cannot be found in META. Parent:" +
+                info.getRegionNameAsString());
           }
         }
       }
@@ -439,8 +448,15 @@
       return processRowInternal(rowResult);
     }
 
-    private Result getRegionResultBlocking(HTable metaTable, long timeout, byte[] regionName)
-        throws IOException {
+    /**
+     * Returns region Result by querying the META table for regionName. It will block until
+     * the region is found in META. It will also check for parent in META to make sure that
+     * if parent is deleted, we no longer have to wait, and should continue (HBASE-8590)
+     * @return Result object is daughter is found, or null if parent is gone from META
+     * @throws TimeoutException if timeout is reached
+     */
+    private Result getRegionResultBlocking(HTable metaTable, long timeout, byte[] parentRegionName, byte[] regionName)
+        throws IOException, TimeoutException {
       boolean logged = false;
       long start = System.currentTimeMillis();
       while (System.currentTimeMillis() - start < timeout) {
@@ -451,6 +467,17 @@
         if (info != null) {
           return result;
         }
+
+        // check whether parent is still there, if not it means we do not need to wait
+        Get parentGet = new Get(parentRegionName);
+        Result parentResult = metaTable.get(parentGet);
+        HRegionInfo parentInfo = Writables.getHRegionInfoOrNull(
+            parentResult.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER));
+        if (parentInfo == null) {
+          // this means that parent is no more (catalog janitor or somebody else deleted it)
+          return null;
+        }
+
         try {
           if (!logged) {
             if (LOG.isDebugEnabled()) {
@@ -464,7 +491,8 @@
           break;
         }
       }
-      return null;
+      throw new TimeoutException("getRegionResultBlocking", start, System.currentTimeMillis(),
+        timeout);
     }
   }
 
Index: src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java	(working copy)
@@ -42,6 +42,16 @@
   }
 
   @Override
+  public void preCreateTableHandler(final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+  }
+
+  @Override
+  public void postCreateTableHandler(final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+  }
+
+  @Override
   public void preDeleteTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName) throws IOException {
   }
@@ -52,11 +62,31 @@
   }
 
   @Override
+  public void preDeleteTableHandler(final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void postDeleteTableHandler(final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
   public void preModifyTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName, HTableDescriptor htd) throws IOException {
   }
 
   @Override
+  public void postModifyTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HTableDescriptor htd) throws IOException {
+  }
+
+  @Override
+  public void preModifyTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HTableDescriptor htd) throws IOException {
+  }
+
+  @Override
   public void postModifyTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName, HTableDescriptor htd) throws IOException {
   }
@@ -72,6 +102,16 @@
   }
 
   @Override
+  public void preAddColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor column) throws IOException {
+  }
+
+  @Override
+  public void postAddColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor column) throws IOException {
+  }
+
+  @Override
   public void preModifyColumn(ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName, HColumnDescriptor descriptor) throws IOException {
   }
@@ -82,6 +122,16 @@
   }
 
   @Override
+  public void preModifyColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor descriptor) throws IOException {
+  }
+
+  @Override
+  public void postModifyColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor descriptor) throws IOException {
+  }
+
+  @Override
   public void preDeleteColumn(ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName, byte[] c) throws IOException {
   }
@@ -92,6 +142,16 @@
   }
 
   @Override
+  public void preDeleteColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, byte[] c) throws IOException {
+  }
+
+  @Override
+  public void postDeleteColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, byte[] c) throws IOException {
+  }
+
+  @Override
   public void preEnableTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName) throws IOException {
   }
@@ -102,6 +162,16 @@
   }
 
   @Override
+  public void preEnableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void postEnableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
   public void preDisableTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName) throws IOException {
   }
@@ -112,6 +182,16 @@
   }
 
   @Override
+  public void preDisableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
+  public void postDisableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName) throws IOException {
+  }
+
+  @Override
   public void preAssign(ObserverContext<MasterCoprocessorEnvironment> ctx,
       HRegionInfo regionInfo) throws IOException {
   }
Index: src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java	(working copy)
@@ -33,7 +33,8 @@
 
   /**
    * Called before a new table is created by
-   * {@link org.apache.hadoop.hbase.master.HMaster}.
+   * {@link org.apache.hadoop.hbase.master.HMaster}.  Called as part of create
+   * table RPC call
    * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
    * @param ctx the environment to interact with the framework and master
    * @param desc the HTableDescriptor for the table
@@ -44,7 +45,8 @@
       HTableDescriptor desc, HRegionInfo[] regions) throws IOException;
 
   /**
-   * Called after the createTable operation has been requested.
+   * Called after the createTable operation has been requested.  Called as part
+   * of create table RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param desc the HTableDescriptor for the table
    * @param regions the initial regions created for the table
@@ -54,8 +56,31 @@
       HTableDescriptor desc, HRegionInfo[] regions) throws IOException;
 
   /**
+   * Called before a new table is created by {@link org.apache.hadoop.hbase.master.HMaster}. Called
+   * as part of create table handler and it is async to the create RPC call. It can't bypass the
+   * default action, e.g., ctx.bypass() won't have effect.
+   * @param ctx the environment to interact with the framework and master
+   * @param desc the HTableDescriptor for the table
+   * @param regions the initial regions created for the table
+   * @throws IOException
+   */
+  void preCreateTableHandler(final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      HTableDescriptor desc, HRegionInfo[] regions) throws IOException;
+
+  /**
+   * Called after the createTable operation has been requested. Called as part of create table RPC
+   * call. Called as part of create table handler and it is async to the create RPC call.
+   * @param ctx the environment to interact with the framework and master
+   * @param desc the HTableDescriptor for the table
+   * @param regions the initial regions created for the table
+   * @throws IOException
+   */
+  void postCreateTableHandler(final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      HTableDescriptor desc, HRegionInfo[] regions) throws IOException;
+
+  /**
    * Called before {@link org.apache.hadoop.hbase.master.HMaster} deletes a
-   * table
+   * table.  Called as part of delete table RPC call.
    * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
@@ -64,7 +89,8 @@
       byte[] tableName) throws IOException;
 
   /**
-   * Called after the deleteTable operation has been requested.
+   * Called after the deleteTable operation has been requested.  Called as part
+   * of delete table RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    */
@@ -72,17 +98,43 @@
       byte[] tableName) throws IOException;
 
   /**
-   * Called prior to modifying a table's properties.
+   * Called before {@link org.apache.hadoop.hbase.master.HMaster} deletes a
+   * table.  Called as part of delete table handler and
+   * it is async to the delete RPC call.
    * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
+   */
+  void preDeleteTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx, byte[] tableName)
+      throws IOException;
+
+  /**
+   * Called after {@link org.apache.hadoop.hbase.master.HMaster} deletes a
+   * table.  Called as part of delete table handler and it is async to the
+   * delete RPC call.
+   * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   */
+  void postDeleteTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx, byte[] tableName)
+      throws IOException;
+
+  /**
+   * Called prior to modifying a table's properties.  Called as part of modify
+   * table RPC call.
+   * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
    * @param htd the HTableDescriptor
    */
   void preModifyTable(final ObserverContext<MasterCoprocessorEnvironment> ctx,
       final byte[] tableName, HTableDescriptor htd) throws IOException;
 
   /**
-   * Called after the modifyTable operation has been requested.
+   * Called after the modifyTable operation has been requested.  Called as part
+   * of modify table RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    * @param htd the HTableDescriptor
@@ -91,16 +143,42 @@
       final byte[] tableName, HTableDescriptor htd) throws IOException;
 
   /**
-   * Called prior to adding a new column family to the table.
+   * Called prior to modifying a table's properties.  Called as part of modify
+   * table handler and it is async to the modify table RPC call.
+   * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
+   * @param htd the HTableDescriptor
+   */
+  void preModifyTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName, HTableDescriptor htd) throws IOException;
+
+  /**
+   * Called after to modifying a table's properties.  Called as part of modify
+   * table handler and it is async to the modify table RPC call.
+   * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   * @param htd the HTableDescriptor
+   */
+  void postModifyTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName, HTableDescriptor htd) throws IOException;
+
+  /**
+   * Called prior to adding a new column family to the table.  Called as part of
+   * add column RPC call.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
    * @param column the HColumnDescriptor
    */
   void preAddColumn(final ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte[] tableName, HColumnDescriptor column) throws IOException;
 
   /**
-   * Called after the new column family has been created.
+   * Called after the new column family has been created.  Called as part of
+   * add column RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    * @param column the HColumnDescriptor
@@ -109,16 +187,40 @@
       byte[] tableName, HColumnDescriptor column) throws IOException;
 
   /**
-   * Called prior to modifying a column family's attributes.
+   * Called prior to adding a new column family to the table.  Called as part of
+   * add column handler.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
+   * @param column the HColumnDescriptor
+   */
+  void preAddColumnHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor column) throws IOException;
+
+  /**
+   * Called after the new column family has been created.  Called as part of
+   * add column handler.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   * @param column the HColumnDescriptor
+   */
+  void postAddColumnHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor column) throws IOException;
+
+  /**
+   * Called prior to modifying a column family's attributes.  Called as part of
+   * modify column RPC call.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
    * @param descriptor the HColumnDescriptor
    */
   void preModifyColumn(final ObserverContext<MasterCoprocessorEnvironment> ctx,
       byte [] tableName, HColumnDescriptor descriptor) throws IOException;
 
   /**
-   * Called after the column family has been updated.
+   * Called after the column family has been updated.  Called as part of modify
+   * column RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    * @param descriptor the HColumnDescriptor
@@ -127,16 +229,40 @@
       byte[] tableName, HColumnDescriptor descriptor) throws IOException;
 
   /**
-   * Called prior to deleting the entire column family.
+   * Called prior to modifying a column family's attributes.  Called as part of
+   * modify column handler.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
+   * @param descriptor the HColumnDescriptor
+   */
+  void preModifyColumnHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor descriptor) throws IOException;
+
+  /**
+   * Called after the column family has been updated.  Called as part of modify
+   * column handler.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   * @param descriptor the HColumnDescriptor
+   */
+  void postModifyColumnHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      byte[] tableName, HColumnDescriptor descriptor) throws IOException;
+
+  /**
+   * Called prior to deleting the entire column family.  Called as part of
+   * delete column RPC call.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
    * @param c the column
    */
   void preDeleteColumn(final ObserverContext<MasterCoprocessorEnvironment> ctx,
       final byte [] tableName, final byte[] c) throws IOException;
 
   /**
-   * Called after the column family has been deleted.
+   * Called after the column family has been deleted.  Called as part of delete
+   * column RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    * @param c the column
@@ -145,7 +271,29 @@
       final byte [] tableName, final byte[] c) throws IOException;
 
   /**
-   * Called prior to enabling a table.
+   * Called prior to deleting the entire column family.  Called as part of
+   * delete column handler.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   * @param c the column
+   */
+  void preDeleteColumnHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName, final byte[] c) throws IOException;
+
+  /**
+   * Called after the column family has been deleted.  Called as part of
+   * delete column handler.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   * @param c the column
+   */
+  void postDeleteColumnHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName, final byte[] c) throws IOException;
+
+  /**
+   * Called prior to enabling a table.  Called as part of enable table RPC call.
    * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
@@ -154,7 +302,8 @@
       final byte[] tableName) throws IOException;
 
   /**
-   * Called after the enableTable operation has been requested.
+   * Called after the enableTable operation has been requested.  Called as part
+   * of enable table RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    */
@@ -162,16 +311,39 @@
       final byte[] tableName) throws IOException;
 
   /**
-   * Called prior to disabling a table.
+   * Called prior to enabling a table.  Called as part of enable table handler
+   * and it is async to the enable table RPC call.
    * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    */
+  void preEnableTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName) throws IOException;
+
+  /**
+   * Called after the enableTable operation has been requested.  Called as part
+   * of enable table handler and it is async to the enable table RPC call.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   */
+  void postEnableTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName) throws IOException;
+
+  /**
+   * Called prior to disabling a table.  Called as part of disable table RPC
+   * call.
+   * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   */
   void preDisableTable(final ObserverContext<MasterCoprocessorEnvironment> ctx,
       final byte[] tableName) throws IOException;
 
   /**
-   * Called after the disableTable operation has been requested.
+   * Called after the disableTable operation has been requested.  Called as part
+   * of disable table RPC call.
    * @param ctx the environment to interact with the framework and master
    * @param tableName the name of the table
    */
@@ -179,6 +351,27 @@
       final byte[] tableName) throws IOException;
 
   /**
+   * Called prior to disabling a table.  Called as part of disable table handler
+   * and it is asyn to the disable table RPC call.
+   * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   */
+  void preDisableTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName) throws IOException;
+
+  /**
+   * Called after the disableTable operation has been requested.  Called as part
+   * of disable table handler and it is asyn to the disable table RPC call.
+   * @param ctx the environment to interact with the framework and master
+   * @param tableName the name of the table
+   */
+  void postDisableTableHandler(
+      final ObserverContext<MasterCoprocessorEnvironment> ctx,
+      final byte[] tableName) throws IOException;
+
+  /**
    * Called prior to moving a given region from one region server to another.
    * @param ctx the environment to interact with the framework and master
    * @param region the HRegionInfo
Index: src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserverExt.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserverExt.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserverExt.java	(revision 0)
@@ -0,0 +1,37 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.coprocessor;
+
+import java.io.IOException;
+
+/**
+ * This is an extension for the MasterObserver interface. The APIs added into this interface are not
+ * exposed by HBase. This is internally being used by CMWH HBase. Customer should not make use of
+ * this interface points. <br>
+ * Note : The APIs in this interface is subject to change at any time.
+ */
+public interface MasterObserverExt {
+
+  /**
+   * Call before the master initialization is set to true.
+   */
+  void preMasterInitialization(final ObserverContext<MasterCoprocessorEnvironment> ctx)
+      throws IOException;
+}
Index: src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserverExt.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserverExt.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserverExt.java	(revision 0)
@@ -0,0 +1,134 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.coprocessor;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.hadoop.hbase.client.Mutation;
+import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.OperationStatus;
+import org.apache.hadoop.hbase.regionserver.SplitTransaction.SplitInfo;
+import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
+import org.apache.hadoop.hbase.util.Pair;
+
+/**
+ * This is an extension for the RegionObserver interface. The APIs added into this interface are not
+ * exposed by HBase. This is internally being used by CMWH HBase. Customer should not make use of
+ * this interface points. <br>
+ * Note : The APIs in this interface is subject to change at any time.
+ */
+public interface RegionObserverExt {
+
+  /**
+   * Internally the Put/Delete are handled as a batch Called before actual put operation starts in
+   * the region.
+   * <p>
+   * Call CoprocessorEnvironment#bypass to skip default actions
+   * <p>
+   * Call CoprocessorEnvironment#complete to skip any subsequent chained coprocessors
+   * @param ctx the environment provided by the region server
+   * @param mutations list of mutations
+   * @param edit The WALEdit object that will be written to the wal
+   * @throws IOException if an error occurred on the coprocessor
+   */
+  void preBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> ctx,
+      final List<Pair<Mutation, OperationStatus>> mutationVsBatchOp, final WALEdit edit)
+      throws IOException;
+
+  /**
+   * Internally the Put/Delete are handled as a batch. Called after actual batch put operation
+   * completes in the region.
+   * <p>
+   * Call CoprocessorEnvironment#bypass to skip default actions
+   * <p>
+   * Call CoprocessorEnvironment#complete to skip any subsequent chained coprocessors
+   * @param ctx the environment provided by the region server
+   * @param mutations list of mutations
+   * @param walEdit The WALEdit object that will be written to the wal
+   */
+  void postBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> ctx,
+      final List<Mutation> mutations, WALEdit walEdit) throws IOException;
+
+  /**
+   * Called after the completion of batch put/delete and will be called even if the batch operation
+   * fails
+   * @param ctx
+   * @param mutations list of mutations
+   * @throws IOException
+   */
+  void postCompleteBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> ctx,
+      List<Mutation> mutations) throws IOException;
+
+  /**
+   * This will be called by the scan flow when the current scanned row is being filtered out by the
+   * filter. The filter may be filtering out the row via any of the below scenarios
+   * <ol>
+   * <li>
+   * <code>boolean filterRowKey(byte [] buffer, int offset, int length)</code> returning true</li>
+   * <li>
+   * <code>boolean filterRow()</code> returning true</li>
+   * <li>
+   * <code>void filterRow(List<KeyValue> kvs)</code> removing all the kvs from the passed List</li>
+   * </ol>
+   * @param ctx the environment provided by the region server
+   * @param s the scanner
+   * @param currentRow The current rowkey which got filtered out.
+   * @return Returns whether more rows are available for the scanner or not.
+   * @throws IOException
+   */
+  boolean postFilterRow(final ObserverContext<RegionCoprocessorEnvironment> ctx,
+      final InternalScanner s, final byte[] currentRow) throws IOException;
+
+  /**
+   * This will be called before PONR step as part of split transaction
+   * @param ctx
+   * @param splitKey
+   * @return
+   * @throws IOException
+   */
+  SplitInfo preSplitBeforePONR(final ObserverContext<RegionCoprocessorEnvironment> ctx,
+      byte[] splitKey) throws IOException;
+
+  /**
+   * This is used to roll back the split related transactions.
+   * @param ctx
+   * @return
+   * @throws IOException
+   */
+  void preRollBack(final ObserverContext<RegionCoprocessorEnvironment> ctx) throws IOException;
+
+  /**
+   * Used after closeRegionOperation in the batchMutate()
+   * @param ctx
+   * @throws IOException
+   */
+  void postCloseRegionOperation(final ObserverContext<RegionCoprocessorEnvironment> ctx)
+      throws IOException;
+
+  /**
+   * Used after startRegionOperation in the batchMutate()
+   * @param ctx
+   * @throws IOException
+   */
+  void postStartRegionOperation(final ObserverContext<RegionCoprocessorEnvironment> ctx)
+      throws IOException;
+
+}
Index: src/main/java/org/apache/hadoop/hbase/filter/DecimalComparator.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/filter/DecimalComparator.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/filter/DecimalComparator.java	(revision 0)
@@ -0,0 +1,69 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.filter;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.filter.BinaryComparator;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class DecimalComparator extends BinaryComparator {
+
+  protected byte[] msb = new byte[1];
+  protected byte[] temp;
+
+  public DecimalComparator() {
+
+  }
+
+  /**
+   * Constructor
+   * @param value value
+   */
+  public DecimalComparator(byte[] value) {
+    super(value);
+    byte b = value[0];
+    msb[0] = (byte) ((b >> 7) & 1);
+    temp = new byte[value.length];
+    System.arraycopy(value, 0, temp, 0, value.length);
+  }
+
+  @Override
+  public int compareTo(byte[] value, int offset, int length) {
+    return super.compareTo(value, offset, length);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    msb = Bytes.readByteArray(in);
+    temp = Bytes.readByteArray(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    Bytes.writeByteArray(out, msb);
+    Bytes.writeByteArray(out, temp);
+  }
+
+}
Index: src/main/java/org/apache/hadoop/hbase/filter/DoubleComparator.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/filter/DoubleComparator.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/filter/DoubleComparator.java	(revision 0)
@@ -0,0 +1,53 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.filter;
+
+import org.apache.hadoop.hbase.index.util.ByteArrayBuilder;
+
+public class DoubleComparator extends DecimalComparator {
+  public DoubleComparator() {
+  }
+
+  public DoubleComparator(byte[] value) {
+    super(value);
+  }
+
+  @Override
+  public int compareTo(byte[] actualValue, int offset, int length) {
+    ByteArrayBuilder val = new ByteArrayBuilder(length);
+    val.put(actualValue, offset, length);
+    byte[] array = val.array();
+    if (msb[0] == 0) {
+      value[0] ^= (1 << 7);
+      array[0] ^= (1 << 7);
+    } else {
+      for (int i = 0; i < 8; i++) {
+        value[i] ^= 0xff;
+      }
+
+      for (int i = 0; i < 8; i++) {
+        array[i] ^= 0xff;
+      }
+    }
+    int compareTo = super.compareTo(array, 0, length);
+    System.arraycopy(temp, 0, value, 0, value.length);
+    return compareTo;
+  }
+}
Index: src/main/java/org/apache/hadoop/hbase/filter/FloatComparator.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/filter/FloatComparator.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/filter/FloatComparator.java	(revision 0)
@@ -0,0 +1,58 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.filter;
+
+import org.apache.hadoop.hbase.index.util.ByteArrayBuilder;
+
+public class FloatComparator extends DecimalComparator {
+
+  public FloatComparator() {
+
+  }
+
+  public FloatComparator(byte[] value) {
+    super(value);
+  }
+
+  @Override
+  public int compareTo(byte[] actualValue, int offset, int length) {
+    ByteArrayBuilder val = new ByteArrayBuilder(length);
+    val.put(actualValue, offset, length);
+    byte[] array = val.array();
+    if (msb[0] == 0) {
+      value[0] ^= (1 << 7);
+      array[0] ^= (1 << 7);
+    } else {
+      value[0] ^= 0xff;
+      value[1] ^= 0xff;
+      value[2] ^= 0xff;
+      value[3] ^= 0xff;
+
+      array[0] ^= 0xff;
+      array[1] ^= 0xff;
+      array[2] ^= 0xff;
+      array[3] ^= 0xff;
+    }
+    int compareTo = super.compareTo(array, 0, length);
+    System.arraycopy(temp, 0, value, 0, value.length);
+    return compareTo;
+  }
+
+}
Index: src/main/java/org/apache/hadoop/hbase/filter/IntComparator.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/filter/IntComparator.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/filter/IntComparator.java	(revision 0)
@@ -0,0 +1,46 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.filter;
+
+import org.apache.hadoop.hbase.index.util.ByteArrayBuilder;
+
+public class IntComparator extends DecimalComparator {
+
+  public IntComparator() {
+
+  }
+
+  public IntComparator(byte[] value) {
+    super(value);
+  }
+
+  @Override
+  public int compareTo(byte[] actualValue, int offset, int length) {
+    ByteArrayBuilder val = new ByteArrayBuilder(length);
+    val.put(actualValue, offset, length);
+    value[0] ^= (1 << 7);
+    byte[] array = val.array();
+    array[0] ^= (1 << 7);
+    int compareTo = super.compareTo(array, 0, length);
+    System.arraycopy(temp, 0, value, 0, value.length);
+    return compareTo;
+  }
+
+}
Index: src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java	(working copy)
@@ -271,6 +271,14 @@
   public HTableDescriptor[] getHTableDescriptors(List<String> tableNames);
 
   /**
+   * Set the state to ENABLED or DISABLED in the zookeeper
+   * @param tableName
+   * @param state
+   * @throws IOException
+   */
+  public void setTableState(String tableName, String state) throws IOException;
+
+  /**
    * Executes a single {@link org.apache.hadoop.hbase.ipc.CoprocessorProtocol}
    * method using the registered protocol handlers.
    * {@link CoprocessorProtocol} implementations must be registered via the
Index: src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java	(working copy)
@@ -23,6 +23,8 @@
 import java.io.DataOutput;
 import java.io.IOException;
 import java.lang.Thread.UncaughtExceptionHandler;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -32,6 +34,7 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 import java.util.NavigableMap;
 import java.util.Set;
 import java.util.SortedMap;
@@ -613,6 +616,7 @@
         regionsInTransition.put(encodedRegionName, new RegionState(regionInfo,
             RegionState.State.OPENING, data.getStamp(), data.getOrigin()));
         failoverProcessedRegions.put(encodedRegionName, regionInfo);
+        putRegionPlan(regionInfo, data.getOrigin());
         break;
 
       case RS_ZK_REGION_OPENED:
@@ -641,7 +645,6 @@
       }
     }
   }
-  
 
   /**
    * Put the region <code>hri</code> into an offline state up in zk.
@@ -1326,6 +1329,10 @@
     clearRegionPlan(regionInfo);
     setOffline(regionInfo);
 
+    // This is needed in case of secondary index load balancer so that the
+    // region needs to be cleared from our map
+    clearRegionPlanFromBalancerPlan(regionInfo);
+
     synchronized(this.regionsInTransition) {
       if (this.regionsInTransition.remove(regionInfo.getEncodedName()) != null) {
         this.regionsInTransition.notifyAll();
@@ -2011,8 +2018,8 @@
           || existingPlan.getDestination() == null
           || drainingServers.contains(existingPlan.getDestination())) {
         newPlan = true;
-        randomPlan = new RegionPlan(state.getRegion(), null, balancer
-            .randomAssignment(servers));
+        randomPlan = new RegionPlan(state.getRegion(), null, balancer.randomAssignment(state
+          .getRegion(), servers));
         this.regionPlans.put(encodedName, randomPlan);
       }
     }
@@ -2328,6 +2335,29 @@
   }
 
   /**
+   * Waits until the specified region has completed assignment or timeout elapsed.
+   * <p>
+   * If the region is already assigned, returns immediately. Otherwise, method blocks until the
+   * region is assigned.
+   * @param regionInfo region to wait on assignment for
+   * @param timeout How long to wait.
+   * @throws InterruptedException
+   */
+  public void waitForAssignment(HRegionInfo regionInfo, long timeout) throws InterruptedException {
+    synchronized (regions) {
+      long startTime = System.currentTimeMillis();
+      long remaining = timeout;
+      while (!regions.containsKey(regionInfo) && remaining > 0) {
+        // We should receive a notification, but it's
+        // better to have a timeout to recheck the condition here:
+        // it lowers the impact of a race condition if any
+        regions.wait(remaining);
+        remaining = timeout - (System.currentTimeMillis() - startTime);
+      }
+    }
+  }
+
+  /**
    * Assigns the ROOT region.
    * <p>
    * Assumes that ROOT is currently closed and is not being actively served by
@@ -2574,7 +2604,7 @@
    * @return True if nothing in regions in transition.
    * @throws InterruptedException
    */
-  boolean waitUntilNoRegionsInTransition(final long timeout)
+  public boolean waitUntilNoRegionsInTransition(final long timeout)
   throws InterruptedException {
     // Blocks until there are no regions in transition. It is possible that
     // there
@@ -2601,7 +2631,7 @@
    * @return True if nothing in regions in transition.
    * @throws InterruptedException
    */
-  boolean waitUntilNoRegionsInTransition(final long timeout, Set<HRegionInfo> regions)
+  public boolean waitUntilNoRegionsInTransition(final long timeout, Set<HRegionInfo> regions)
   throws InterruptedException {
     // Blocks until there are no regions in transition.
     long startTime = System.currentTimeMillis();
@@ -2713,6 +2743,7 @@
             regions.put(regionInfo, regionLocation);
             addToServers(regionLocation, regionInfo);
           }
+          putRegionPlan(regionInfo, regionLocation);
         }
         disablingOrEnabling = addTheTablesInPartialState(regionInfo);
         if (enabling) {
@@ -3382,7 +3413,7 @@
   Map<String, Map<ServerName, List<HRegionInfo>>> getAssignmentsByTable() {
     Map<String, Map<ServerName, List<HRegionInfo>>> result = null;
     synchronized (this.regions) {
-      result = new HashMap<String, Map<ServerName,List<HRegionInfo>>>();
+      result = new TreeMap<String, Map<ServerName,List<HRegionInfo>>>();
       if (!this.master.getConfiguration().
           getBoolean("hbase.master.loadbalance.bytable", true)) {
         result.put("ensemble", getAssignments());
@@ -3665,4 +3696,42 @@
     }
   }
 
+  protected void setDisabledTable(String tableName) {
+    try {
+      this.zkTable.setDisabledTable(tableName);
+    } catch (KeeperException e) {
+      // here we can abort as it is the start up flow
+      String errorMsg =
+          "Unable to ensure that the table " + tableName + " will be"
+              + " enabled because of a ZooKeeper issue";
+      LOG.error(errorMsg);
+      this.master.abort(errorMsg, e);
+    }
+  }
+
+  public void putRegionPlan(HRegionInfo hri, ServerName dest) {
+    try {
+      if (LoadBalancerFactory.secIndexLoadBalancerKlass.isInstance(this.balancer)) {
+        Method method =
+            LoadBalancerFactory.secIndexLoadBalancerKlass.getMethod("putRegionPlan",
+              HRegionInfo.class, ServerName.class);
+        method.invoke(this.balancer, hri, dest);
+      }
+    } catch (Throwable t) {
+      // Nothing to do
+    }
+  }
+
+  public void clearRegionPlanFromBalancerPlan(HRegionInfo hri) {
+    try {
+      if (LoadBalancerFactory.secIndexLoadBalancerKlass.isInstance(this.balancer)) {
+        Method method =
+            LoadBalancerFactory.secIndexLoadBalancerKlass.getMethod(
+              "clearRegionInfoFromRegionPlan", HRegionInfo.class);
+        method.invoke(this.balancer, hri);
+      }
+    } catch (Throwable t) {
+      // Nothing to do
+    }
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java	(working copy)
@@ -768,7 +768,7 @@
     return assignments;
   }
 
-  public ServerName randomAssignment(List<ServerName> servers) {
+  public ServerName randomAssignment(HRegionInfo regionInfo, List<ServerName> servers) {
     if (servers == null || servers.isEmpty()) {
       LOG.warn("Wanted to do random assignment but no servers to assign to");
       return null;
Index: src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java	(working copy)
@@ -40,6 +40,8 @@
 import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.executor.EventHandler;
 import org.apache.hadoop.hbase.master.AssignmentManager;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.master.ServerManager;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
@@ -124,7 +126,14 @@
     String tableName = this.hTableDescriptor.getNameAsString();
     try {
       LOG.info("Attempting to create the table " + tableName);
+      MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+      if (cpHost != null) {
+        cpHost.preCreateTableHandler(this.hTableDescriptor, this.newRegions);
+      }
       handleCreateTable(tableName);
+      if (cpHost != null) {
+        cpHost.postCreateTableHandler(this.hTableDescriptor, this.newRegions);
+      }
       completed(null);
     } catch (Throwable e) {
       LOG.error("Error trying to create the table " + tableName, e);
Index: src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java	(working copy)
@@ -32,6 +32,8 @@
 import org.apache.hadoop.hbase.backup.HFileArchiver;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.master.AssignmentManager;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -52,6 +54,10 @@
   @Override
   protected void handleTableOperation(List<HRegionInfo> regions)
   throws IOException, KeeperException {
+    MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+    if (cpHost != null) {
+      cpHost.preDeleteTableHandler(this.tableName);
+    }
     // 1. Wait because of region in transition
     AssignmentManager am = this.masterServices.getAssignmentManager();
     long waitTime = server.getConfiguration().
@@ -99,6 +105,9 @@
 
       // 7. If entry for this table in zk, and up in AssignmentManager, remove it.
       am.getZKTable().setDeletedTable(Bytes.toString(tableName));
+      if (cpHost != null) {
+        cpHost.postDeleteTableHandler(this.tableName);
+      }
     }
   }
 
Index: src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java	(working copy)
@@ -34,6 +34,8 @@
 import org.apache.hadoop.hbase.executor.EventHandler;
 import org.apache.hadoop.hbase.master.AssignmentManager;
 import org.apache.hadoop.hbase.master.BulkAssigner;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.zookeeper.KeeperException;
 
@@ -95,7 +97,14 @@
   public void process() {
     try {
       LOG.info("Attemping to disable table " + this.tableNameStr);
+      MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+      if (cpHost != null) {
+        cpHost.preDisableTableHandler(this.tableName);
+      }
       handleDisableTable();
+      if (cpHost != null) {
+        cpHost.postDisableTableHandler(this.tableName);
+      }
     } catch (IOException e) {
       LOG.error("Error trying to disable table " + this.tableNameStr, e);
     } catch (KeeperException e) {
Index: src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java	(working copy)
@@ -37,6 +37,8 @@
 import org.apache.hadoop.hbase.master.AssignmentManager;
 import org.apache.hadoop.hbase.master.BulkAssigner;
 import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.ServerManager;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -113,7 +115,14 @@
   public void process() {
     try {
       LOG.info("Attemping to enable the table " + this.tableNameStr);
+      MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+      if (cpHost != null) {
+        cpHost.preEnableTableHandler(this.tableName);
+      }
       handleEnableTable();
+      if (cpHost != null) {
+        cpHost.postEnableTableHandler(this.tableName);
+      }
     } catch (IOException e) {
       LOG.error("Error trying to enable the table " + this.tableNameStr, e);
     } catch (KeeperException e) {
@@ -180,6 +189,7 @@
           regions.add(hri);
           if (sn != null && serverManager.isServerOnline(sn)) {
             this.assignmentManager.addPlan(hri.getEncodedName(), new RegionPlan(hri, null, sn));
+            this.assignmentManager.putRegionPlan(hri, sn);
           }
         }
       } else if (onlineRegions.contains(hri)) {
Index: src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java	(working copy)
@@ -22,9 +22,11 @@
 import java.io.IOException;
 import java.util.List;
 
+import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterServices;
 
 public class ModifyTableHandler extends TableEventHandler {
@@ -44,8 +46,15 @@
   @Override
   protected void handleTableOperation(List<HRegionInfo> hris)
   throws IOException {
+    MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+    if (cpHost != null) {
+      cpHost.preModifyTableHandler(this.tableName, this.htd);
+    }
     // Update descriptor
     this.masterServices.getTableDescriptors().add(this.htd);
+    if (cpHost != null) {
+      cpHost.postModifyTableHandler(this.tableName, this.htd);
+    }
   }
 
   @Override
Index: src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java	(working copy)
@@ -27,6 +27,8 @@
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.InvalidFamilyOperationException;
 import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterServices;
 
 /**
@@ -50,9 +52,17 @@
   @Override
   protected void handleTableOperation(List<HRegionInfo> hris)
   throws IOException {
+    MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+    if (cpHost != null) {
+      cpHost.preAddColumnHandler(this.tableName, this.familyDesc);
+    }
     // Update table descriptor
     this.masterServices.getMasterFileSystem().addColumn(tableName, familyDesc);
+    if (cpHost != null) {
+      cpHost.postAddColumnHandler(this.tableName, this.familyDesc);
+    }
   }
+  
   @Override
   public String toString() {
     String name = "UnknownServerName";
Index: src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java	(working copy)
@@ -25,6 +25,8 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -45,6 +47,10 @@
 
   @Override
   protected void handleTableOperation(List<HRegionInfo> hris) throws IOException {
+    MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+    if (cpHost != null) {
+      cpHost.preDeleteColumnHandler(this.tableName, this.familyName);
+    }
     MasterFileSystem mfs = this.masterServices.getMasterFileSystem();
     // Update table descriptor
     mfs.deleteColumn(tableName, familyName);
@@ -53,6 +59,9 @@
       // Delete the family directory in FS for all the regions one by one
       mfs.deleteFamilyFromFS(hri, familyName);
     }
+    if (cpHost != null) {
+      cpHost.postDeleteColumnHandler(this.tableName, this.familyName);
+    }
   }
 
   @Override
Index: src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java	(working copy)
@@ -27,6 +27,8 @@
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.InvalidFamilyOperationException;
 import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.util.Bytes;
 
@@ -47,11 +49,18 @@
 
   @Override
   protected void handleTableOperation(List<HRegionInfo> regions) throws IOException {
+    MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+    if (cpHost != null) {
+      cpHost.preModifyColumnHandler(this.tableName, this.familyDesc);
+    }
     // Update table descriptor in HDFS
     HTableDescriptor htd =
       this.masterServices.getMasterFileSystem().modifyColumn(tableName, familyDesc);
     // Update in-memory descriptor cache
     this.masterServices.getTableDescriptors().add(htd);
+    if (cpHost != null) {
+      cpHost.postModifyColumnHandler(this.tableName, this.familyDesc);
+    }
   }
 
   @Override
Index: src/main/java/org/apache/hadoop/hbase/master/HMaster.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/HMaster.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/HMaster.java	(working copy)
@@ -120,6 +120,7 @@
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperListener;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.hadoop.hbase.zookeeper.ZKTable.TableState;
 import org.apache.hadoop.io.MapWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.metrics.util.MBeanUtil;
@@ -655,10 +656,9 @@
     fixupDaughters(status);
 
     if (!masterRecovery) {
-      // Start balancer and meta catalog janitor after meta and regions have
+      // Start meta catalog janitor after meta and regions have
       // been assigned.
-      status.setStatus("Starting balancer and catalog janitor");
-      this.balancerChore = getAndStartBalancerChore(this);
+      status.setStatus("Starting catalog janitor");
       this.catalogJanitorChore = new CatalogJanitor(this, this);
       startCatalogJanitorChore();
       registerMBean();
@@ -666,6 +666,17 @@
 
     status.markComplete("Initialization successful");
     LOG.info("Master has completed initialization");
+    if (this.cpHost != null) {
+      try {
+        this.cpHost.preMasterInitialization();
+      } catch (IOException e) {
+        LOG.error("Coprocessor preMasterInitialization() hook failed", e);
+      }
+    }
+    if (!masterRecovery) {
+      status.setStatus("Starting balancer");
+      this.balancerChore = getAndStartBalancerChore(this);
+    }
     initialized = true;
 
     // clear the dead servers with same host name and port of online server because we are not
@@ -1136,6 +1147,10 @@
     }
     // If balance not true, don't run balancer.
     if (!this.balanceSwitch) return false;
+    return balanceInternals();
+  }
+
+  public boolean balanceInternals() {
     // Do this call outside of synchronized block.
     int maximumBalanceTime = getBalancerCutoffTime();
     long cutoffTime = System.currentTimeMillis() + maximumBalanceTime;
@@ -1272,9 +1287,11 @@
     if (destServerName == null || destServerName.length == 0) {
       LOG.info("Passed destination servername is null or empty so choosing a server at random");
       List<ServerName> destServers = this.serverManager.getOnlineServersList();
-      destServers.remove(p.getSecond());
+      if (destServers.size() > 1) {
+        destServers.remove(p.getSecond());
+      }
       // If i have only one RS then destination can be null.
-      dest = balancer.randomAssignment(destServers);
+      dest = balancer.randomAssignment(p.getFirst(), destServers);
     } else {
       dest = new ServerName(Bytes.toString(destServerName));
     }
@@ -2089,6 +2106,37 @@
     LOG.info("Registered HMaster MXBean");
   }
 
+  public LoadBalancer getBalancer() {
+    return this.balancer;
+  }
+
+
+  @Override
+  public void setTableState(String tableName, String state) throws IOException {
+    TableState tableState = TableState.valueOf(state);
+    if (tableState == TableState.ENABLED) {
+      List<HRegionInfo> tableRegions =
+          MetaReader.getTableRegions(this.catalogTracker, Bytes.toBytes(tableName));
+      List<HRegionInfo> onlineRegions =
+          this.assignmentManager.getRegionsOfTable(Bytes.toBytes(tableName));
+      if (tableRegions.size() == onlineRegions.size()) {
+        this.assignmentManager.setEnabledTable(tableName);
+      } else {
+        LOG.warn("All the regions for the table are not assigned. Cannot set table " + tableName
+            + " as ENABLED.");
+      }
+    } else {
+      List<HRegionInfo> onlineRegions =
+          this.assignmentManager.getRegionsOfTable(Bytes.toBytes(tableName));
+      if (onlineRegions.size() == 0) {
+        this.assignmentManager.setDisabledTable(tableName);
+      } else {
+        LOG.warn("All the regions for the table are not unassigned. Cannot set table " + tableName
+            + " as DISABLED.");
+      }
+    }
+  }
+
   /**
    * Exposed for Testing!
    * @return the current hfile cleaner
Index: src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java	(working copy)
@@ -91,8 +91,9 @@
 
   /**
    * Get a random region server from the list
+   * @param regionInfo Region for which this selection is being done.
    * @param servers
    * @return Servername
    */
-  public ServerName randomAssignment(List<ServerName> servers);
+  public ServerName randomAssignment(HRegionInfo regionInfo, List<ServerName> servers);
 }
Index: src/main/java/org/apache/hadoop/hbase/master/LoadBalancerFactory.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/LoadBalancerFactory.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/LoadBalancerFactory.java	(working copy)
@@ -20,6 +20,10 @@
 
 package org.apache.hadoop.hbase.master;
 
+import java.lang.reflect.Method;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.util.ReflectionUtils;
@@ -29,18 +33,40 @@
  */
 public class LoadBalancerFactory {
 
+  private static final Log LOG = LogFactory.getLog(LoadBalancerFactory.class);
+
+  static Class<?> secIndexLoadBalancerKlass = LoadBalancer.class;
+
   /**
    * Create a loadblanacer from the given conf.
    * @param conf
    * @return A {@link LoadBalancer}
    */
   public static LoadBalancer getLoadBalancer(Configuration conf) {
+    // Create the balancer
+    Class<? extends LoadBalancer> balancerKlass =
+        conf.getClass(HConstants.HBASE_MASTER_LOADBALANCER_CLASS, DefaultLoadBalancer.class,
+          LoadBalancer.class);
+    boolean secondaryIndex = conf.getBoolean("hbase.use.secondary.index", false);
+    String secIndexBalancer = conf.get("hbase.index.loadbalancer.class");
+    if (secondaryIndex) {
+      if (secIndexBalancer == null) {
+        throw new RuntimeException(
+            "Secondary index load  balancer not configured. Configure the property hbase.index.loadbalancer.class");
+      }
+      try {
+        secIndexLoadBalancerKlass = Class.forName(secIndexBalancer.trim());
+        Object secIndexLoadBalancerInstance = secIndexLoadBalancerKlass.newInstance();
+        Method method = secIndexLoadBalancerKlass.getMethod("setDelegator", LoadBalancer.class);
+        method.invoke(secIndexLoadBalancerInstance,
+          (LoadBalancer) ReflectionUtils.newInstance(balancerKlass, conf));
+        return (LoadBalancer) secIndexLoadBalancerInstance;
+      } catch (Throwable t) {
+        LOG.error("Error while initializing/invoking method of seconday index load balancer.", t);
+        throw new RuntimeException("Not able to load the secondary index load balancer class.");
+      }
 
-    // Create the balancer
-    Class<? extends LoadBalancer> balancerKlass = conf.getClass(
-        HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
-        DefaultLoadBalancer.class, LoadBalancer.class);
+    }
     return ReflectionUtils.newInstance(balancerKlass, conf);
-
   }
 }
Index: src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java	(working copy)
@@ -88,7 +88,7 @@
   }
 
   /* Implementation of hooks for invoking MasterObservers */
-  void preCreateTable(HTableDescriptor htd, HRegionInfo[] regions)
+  public void preCreateTable(HTableDescriptor htd, HRegionInfo[] regions)
     throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -106,7 +106,7 @@
     }
   }
 
-  void postCreateTable(HTableDescriptor htd, HRegionInfo[] regions)
+  public void postCreateTable(HTableDescriptor htd, HRegionInfo[] regions)
     throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -124,8 +124,43 @@
     }
   }
 
-  void preDeleteTable(byte[] tableName) throws IOException {
+  public void preCreateTableHandler(HTableDescriptor htd, HRegionInfo[] regions) throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preCreateTableHandler(ctx, htd, regions);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void postCreateTableHandler(HTableDescriptor htd, HRegionInfo[] regions)
+      throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postCreateTableHandler(ctx, htd, regions);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void preDeleteTable(byte[] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
       if (env.getInstance() instanceof MasterObserver) {
         ctx = ObserverContext.createAndPrepare(env, ctx);
@@ -141,7 +176,7 @@
     }
   }
 
-  void postDeleteTable(byte[] tableName) throws IOException {
+  public void postDeleteTable(byte[] tableName) throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
       if (env.getInstance() instanceof MasterObserver) {
@@ -158,7 +193,41 @@
     }
   }
 
-  void preModifyTable(final byte[] tableName, HTableDescriptor htd)
+  public void preDeleteTableHandler(byte[] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preDeleteTableHandler(ctx, tableName);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void postDeleteTableHandler(byte[] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postDeleteTableHandler(ctx, tableName);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void preModifyTable(final byte[] tableName, HTableDescriptor htd)
       throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -177,7 +246,7 @@
     }
   }
 
-  void postModifyTable(final byte[] tableName, HTableDescriptor htd)
+  public void postModifyTable(final byte[] tableName, HTableDescriptor htd)
       throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -196,8 +265,44 @@
     }
   }
 
-  boolean preAddColumn(byte [] tableName, HColumnDescriptor column)
+  public void preModifyTableHandler(final byte[] tableName, HTableDescriptor htd)
       throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preModifyTableHandler(ctx, tableName, htd);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void postModifyTableHandler(final byte[] tableName, HTableDescriptor htd)
+      throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postModifyTableHandler(ctx, tableName, htd);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public boolean preAddColumn(byte [] tableName, HColumnDescriptor column)
+      throws IOException {
     boolean bypass = false;
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -217,7 +322,7 @@
     return bypass;
   }
 
-  void postAddColumn(byte [] tableName, HColumnDescriptor column)
+  public void postAddColumn(byte [] tableName, HColumnDescriptor column)
       throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -236,7 +341,44 @@
     }
   }
 
-  boolean preModifyColumn(byte [] tableName, HColumnDescriptor descriptor)
+  public boolean preAddColumnHandler(byte[] tableName, HColumnDescriptor column) throws IOException {
+    boolean bypass = false;
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preAddColumnHandler(ctx, tableName, column);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        bypass |= ctx.shouldBypass();
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+    return bypass;
+  }
+
+  public void postAddColumnHandler(byte[] tableName, HColumnDescriptor column) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postAddColumnHandler(ctx, tableName, column);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public boolean preModifyColumn(byte [] tableName, HColumnDescriptor descriptor)
       throws IOException {
     boolean bypass = false;
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
@@ -258,7 +400,7 @@
     return bypass;
   }
 
-  void postModifyColumn(byte [] tableName, HColumnDescriptor descriptor)
+  public void postModifyColumn(byte [] tableName, HColumnDescriptor descriptor)
       throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -277,6 +419,45 @@
     }
   }
 
+  public boolean preModifyColumnHandler(byte[] tableName, HColumnDescriptor descriptor)
+      throws IOException {
+    boolean bypass = false;
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preModifyColumnHandler(ctx, tableName, descriptor);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        bypass |= ctx.shouldBypass();
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+    return bypass;
+  }
+
+  public void postModifyColumnHandler(byte[] tableName, HColumnDescriptor descriptor)
+      throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postModifyColumnHandler(ctx, tableName, descriptor);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
   boolean preDeleteColumn(final byte [] tableName, final byte [] c)
       throws IOException {
     boolean bypass = false;
@@ -298,7 +479,7 @@
     return bypass;
   }
 
-  void postDeleteColumn(final byte [] tableName, final byte [] c)
+  public void postDeleteColumn(final byte [] tableName, final byte [] c)
       throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -317,8 +498,45 @@
     }
   }
 
-  void preEnableTable(final byte [] tableName) throws IOException {
+  public boolean preDeleteColumnHandler(final byte[] tableName, final byte[] c) throws IOException {
+    boolean bypass = false;
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preDeleteColumnHandler(ctx, tableName, c);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        bypass |= ctx.shouldBypass();
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+    return bypass;
+  }
+
+  public void postDeleteColumnHandler(final byte[] tableName, final byte[] c) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postDeleteColumnHandler(ctx, tableName, c);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void preEnableTable(final byte [] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
       if (env.getInstance() instanceof MasterObserver) {
         ctx = ObserverContext.createAndPrepare(env, ctx);
@@ -334,7 +552,7 @@
     }
   }
 
-  void postEnableTable(final byte [] tableName) throws IOException {
+  public void postEnableTable(final byte [] tableName) throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
       if (env.getInstance() instanceof MasterObserver) {
@@ -351,8 +569,42 @@
     }
   }
 
-  void preDisableTable(final byte [] tableName) throws IOException {
+  public void preEnableTableHandler(final byte[] tableName) throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preEnableTableHandler(ctx, tableName);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void postEnableTableHandler(final byte[] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postEnableTableHandler(ctx, tableName);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void preDisableTable(final byte [] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
       if (env.getInstance() instanceof MasterObserver) {
         ctx = ObserverContext.createAndPrepare(env, ctx);
@@ -368,7 +620,7 @@
     }
   }
 
-  void postDisableTable(final byte [] tableName) throws IOException {
+  public void postDisableTable(final byte[] tableName) throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
       if (env.getInstance() instanceof MasterObserver) {
@@ -385,8 +637,42 @@
     }
   }
 
-  boolean preMove(final HRegionInfo region, final ServerName srcServer, final ServerName destServer)
-      throws IOException {
+  public void preDisableTableHandler(final byte[] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).preDisableTableHandler(ctx, tableName);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void postDisableTableHandler(final byte[] tableName) throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserver) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserver) env.getInstance()).postDisableTableHandler(ctx, tableName);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public boolean preMove(final HRegionInfo region, final ServerName srcServer,
+      final ServerName destServer) throws IOException {
     boolean bypass = false;
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
@@ -407,8 +693,8 @@
     return bypass;
   }
 
-  void postMove(final HRegionInfo region, final ServerName srcServer, final ServerName destServer)
-      throws IOException {
+  public void postMove(final HRegionInfo region, final ServerName srcServer,
+      final ServerName destServer) throws IOException {
     ObserverContext<MasterCoprocessorEnvironment> ctx = null;
     for (MasterEnvironment env: coprocessors) {
       if (env.getInstance() instanceof MasterObserver) {
@@ -772,4 +1058,21 @@
       }
     }
   }
+
+  public void preMasterInitialization() throws IOException {
+    ObserverContext<MasterCoprocessorEnvironment> ctx = null;
+    for (MasterEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof MasterObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((MasterObserverExt) env.getInstance()).preMasterInitialization(ctx);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java	(working copy)
@@ -176,6 +176,14 @@
         " not splittable because midkey=null");
       return;
     }
+    boolean indexUsed = this.conf.getBoolean("hbase.use.secondary.index", false);
+    if (indexUsed) {
+      if (r.getRegionInfo().getTableNameAsString().endsWith("_idx")) {
+        LOG.warn("Split issued on the index region which is not allowed."
+            + "Returning without splitting the region.");
+        return;
+      }
+    }
     try {
       this.splits.execute(new SplitRequest(r, midKey, this.server));
       if (LOG.isDebugEnabled()) {
Index: src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(working copy)
@@ -1880,8 +1880,7 @@
 
       try {
         // All edits for the given row (across all column families) must happen atomically.
-        prepareDelete(delete);
-        internalDelete(delete, delete.getClusterId(), writeToWAL);
+        doBatchMutate(delete, lid);
       } finally {
         if(lockid == null) releaseRowLock(lid);
       }
@@ -1898,11 +1897,11 @@
    */
   void delete(Map<byte[], List<KeyValue>> familyMap, UUID clusterId,
       boolean writeToWAL) throws IOException {
-    Delete delete = new Delete();
+    Delete delete = new Delete(new byte[0]);
     delete.setFamilyMap(familyMap);
     delete.setClusterId(clusterId);
     delete.setWriteToWAL(writeToWAL);
-    internalDelete(delete, clusterId, writeToWAL);
+    doBatchMutate(delete, null);
   }
 
   /**
@@ -1960,65 +1959,6 @@
   }
 
   /**
-   * @param delete The Delete command
-   * @param clusterId UUID of the originating cluster (for replication).
-   * @param writeToWAL
-   * @throws IOException
-   */
-  private void internalDelete(Delete delete, UUID clusterId,
-      boolean writeToWAL) throws IOException {
-    Map<byte[], List<KeyValue>> familyMap = delete.getFamilyMap();
-    WALEdit walEdit = new WALEdit();
-    /* Run coprocessor pre hook outside of locks to avoid deadlock */
-    if (coprocessorHost != null) {
-      if (coprocessorHost.preDelete(delete, walEdit, writeToWAL)) {
-        return;
-      }
-    }
-
-    long now = EnvironmentEdgeManager.currentTimeMillis();
-    byte [] byteNow = Bytes.toBytes(now);
-    boolean flush = false;
-
-    lock(updatesLock.readLock());
-    try {
-      prepareDeleteTimestamps(delete.getFamilyMap(), byteNow);
-
-      if (writeToWAL) {
-        // write/sync to WAL should happen before we touch memstore.
-        //
-        // If order is reversed, i.e. we write to memstore first, and
-        // for some reason fail to write/sync to commit log, the memstore
-        // will contain uncommitted transactions.
-        //
-        // bunch up all edits across all column families into a
-        // single WALEdit.
-        addFamilyMapToWALEdit(familyMap, walEdit);
-        this.log.append(regionInfo, this.htableDescriptor.getName(),
-            walEdit, clusterId, now, this.htableDescriptor);
-      }
-
-      // Now make changes to the memstore.
-      long addedSize = applyFamilyMapToMemstore(familyMap, null);
-      flush = isFlushSize(this.addAndGetGlobalMemstoreSize(addedSize));
-
-    } finally {
-      this.updatesLock.readLock().unlock();
-    }
-    // do after lock
-    if (coprocessorHost != null) {
-      coprocessorHost.postDelete(delete, walEdit, writeToWAL);
-    }
-    final long after = EnvironmentEdgeManager.currentTimeMillis();
-    this.opMetrics.updateDeleteMetrics(familyMap.keySet(), after-now);
-
-    if (flush) {
-      // Request a cache flush.  Do it outside update lock.
-      requestFlush();
-    }
-  }
-
-  /**
    * @param put
    * @throws IOException
    */
@@ -2078,7 +2018,7 @@
 
       try {
         // All edits for the given row (across all column families) must happen atomically.
-        internalPut(put, put.getClusterId(), writeToWAL);
+        doBatchMutate(put, lid);
       } finally {
         if(lockid == null) releaseRowLock(lid);
       }
@@ -2163,6 +2103,10 @@
       long newSize;
       startRegionOperation();
 
+      if (coprocessorHost != null) {
+        coprocessorHost.postStartRegionOperation();
+      }
+
       try {
         if (!initialized) {
           this.writeRequestsCount.increment();
@@ -2174,6 +2118,9 @@
         newSize = this.addAndGetGlobalMemstoreSize(addedSize);
       } finally {
         closeRegionOperation();
+        if (coprocessorHost != null) {
+          coprocessorHost.postCloseRegionOperation();
+        }
       }
       if (isFlushSize(newSize)) {
         requestFlush();
@@ -2182,6 +2129,46 @@
     return batchOp.retCodeDetails;
   }
 
+  /**
+   * Not to be used by external users. This is only for the IndexRegionObserver.
+   * @param mutationsAndLocks
+   * @return
+   * @throws IOException
+   */
+  public OperationStatus[] batchMutateForIndex(Pair<Mutation, Integer>[] mutationsAndLocks)
+      throws IOException {
+    BatchOperationInProgress<Pair<Mutation, Integer>> batchOp =
+        new BatchOperationInProgress<Pair<Mutation, Integer>>(mutationsAndLocks);
+
+    boolean initialized = false;
+
+    while (!batchOp.isDone()) {
+      long newSize;
+
+      if (coprocessorHost != null) {
+        coprocessorHost.postStartRegionOperation();
+      }
+
+      try {
+        if (!initialized) {
+          this.writeRequestsCount.increment();
+          doPreMutationHook(batchOp);
+          initialized = true;
+        }
+        long addedSize = doMiniBatchMutation(batchOp);
+        newSize = this.addAndGetGlobalMemstoreSize(addedSize);
+      } finally {
+        if (coprocessorHost != null) {
+          coprocessorHost.postCloseRegionOperation();
+        }
+      }
+      if (isFlushSize(newSize)) {
+        requestFlush();
+      }
+    }
+    return batchOp.retCodeDetails;
+  }
+
   private void doPreMutationHook(BatchOperationInProgress<Pair<Mutation, Integer>> batchOp)
       throws IOException {
     /* Run coprocessor pre hook outside of locks to avoid deadlock */
@@ -2372,12 +2359,35 @@
       // ----------------------------------
       w = mvcc.beginMemstoreInsert();
 
-      // calling the pre CP hook for batch mutation
+      // ------------------------------------------------
+      // Call the coprocessor hook to do the actual put.
+      // Here the walDdit is updated with the actual timestamp
+      // for the kv.
+      // This hook will be called for all the puts that are currently
+      // acquired the locks
+      // -------------------------------------------------
       if (coprocessorHost != null) {
-        MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp = 
-          new MiniBatchOperationInProgress<Pair<Mutation, Integer>>(batchOp.operations, 
-          batchOp.retCodeDetails, batchOp.walEditsFromCoprocessors, firstIndex, lastIndexExclusive);
-        if (coprocessorHost.preBatchMutate(miniBatchOp)) return 0L;
+        List<Pair<Mutation, OperationStatus>> mutationVsBatchOp =
+            new ArrayList<Pair<Mutation, OperationStatus>>(lastIndexExclusive - firstIndex);
+        for (int i = firstIndex; i < lastIndexExclusive; i++) {
+          mutationVsBatchOp.add(new Pair<Mutation, OperationStatus>(batchOp.operations[i]
+              .getFirst(), batchOp.retCodeDetails[i]));
+        }
+        if (coprocessorHost.preBatchMutate(mutationVsBatchOp, walEdit)) {
+          return 0;
+        }
+        boolean fullBatchFailed = true;
+        for (int i = 0; i < mutationVsBatchOp.size(); i++) {
+          OperationStatus opStatus = mutationVsBatchOp.get(i).getSecond();
+          if (opStatus.getOperationStatusCode() != OperationStatusCode.NOT_RUN) {
+            batchOp.retCodeDetails[firstIndex + i] = mutationVsBatchOp.get(i).getSecond();
+          } else {
+            fullBatchFailed = false;
+          }
+        }
+        if (fullBatchFailed) {
+          return 0;
+        }
       }
 
       // ------------------------------------
@@ -2460,12 +2470,19 @@
         syncOrDefer(txid, durability);
       }
       walSyncSuccessful = true;
-      // calling the post CP hook for batch mutation
+
+      // hook to complete the actual put
       if (coprocessorHost != null) {
-        MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp = 
-          new MiniBatchOperationInProgress<Pair<Mutation, Integer>>(batchOp.operations, 
-          batchOp.retCodeDetails, batchOp.walEditsFromCoprocessors, firstIndex, lastIndexExclusive);
-        coprocessorHost.postBatchMutate(miniBatchOp);
+        List<Mutation> mutations = new ArrayList<Mutation>();
+        for (int i = firstIndex; i < lastIndexExclusive; i++) {
+          // only for successful puts
+          if (batchOp.retCodeDetails[i].getOperationStatusCode() != OperationStatusCode.SUCCESS) {
+            continue;
+          }
+          Mutation m = batchOp.operations[i].getFirst();
+          mutations.add(m);
+        }
+        coprocessorHost.postBatchMutate(mutations, walEdit);
       }
       
       // ------------------------------------------------------------------
@@ -2515,6 +2532,21 @@
         }
       }
 
+      // call the coprocessor hook to do to any finalization steps
+      // after the put is done
+      if (coprocessorHost != null) {
+        List<Mutation> mutations = new ArrayList<Mutation>();
+        for (int i = firstIndex; i < lastIndexExclusive; i++) {
+          // only for successful puts
+          if (batchOp.retCodeDetails[i].getOperationStatusCode() != OperationStatusCode.SUCCESS) {
+            continue;
+          }
+          Mutation m = batchOp.operations[i].getFirst();
+          mutations.add(m);
+        }
+        coprocessorHost.postCompleteBatchMutate(mutations);
+      }
+
       // do after lock
       final long netTimeMs = EnvironmentEdgeManager.currentTimeMillis()- startTimeMs;
 
@@ -2658,13 +2690,7 @@
           // Using default cluster id, as this can only happen in the
           // originating cluster. A slave cluster receives the result as a Put
           // or Delete
-          if (isPut) {
-            internalPut(((Put) w), HConstants.DEFAULT_CLUSTER_ID, writeToWAL);
-          } else {
-            Delete d = (Delete)w;
-            prepareDelete(d);
-            internalDelete(d, HConstants.DEFAULT_CLUSTER_ID, writeToWAL);
-          }
+          doBatchMutate((Mutation) w, lid);
           return true;
         }
         return false;
@@ -2676,6 +2702,15 @@
     }
   }
 
+  private void doBatchMutate(Mutation mutation, Integer lid) throws IOException,
+      DoNotRetryIOException {
+    Pair<Mutation, Integer> mutateWithLocks[] = new Pair[1];
+    mutateWithLocks[0] = new Pair<Mutation, Integer>(mutation, lid);
+    OperationStatus[] batchMutate = this.batchMutate(mutateWithLocks);
+    if (batchMutate[0].getOperationStatusCode().equals(OperationStatusCode.SANITY_CHECK_FAILURE)) {
+      throw new DoNotRetryIOException(batchMutate[0].getExceptionMsg());
+    }
+  }
 
   /**
    * Complete taking the snapshot on the region. Writes the region info and adds references to the
@@ -2763,7 +2798,7 @@
    * this and the synchronize on 'this' inside in internalFlushCache to send
    * the notify.
    */
-  private void checkResources()
+  public void checkResources()
       throws RegionTooBusyException, InterruptedIOException {
 
     // If catalog region, do not impose resource constraints or block updates.
@@ -2837,8 +2872,7 @@
    * @praram now
    * @throws IOException
    */
-  private void put(byte [] family, List<KeyValue> edits)
-  throws IOException {
+  private void put(byte[] family, List<KeyValue> edits, Integer lid) throws IOException {
     Map<byte[], List<KeyValue>> familyMap;
     familyMap = new HashMap<byte[], List<KeyValue>>();
 
@@ -2847,70 +2881,10 @@
     p.setFamilyMap(familyMap);
     p.setClusterId(HConstants.DEFAULT_CLUSTER_ID);
     p.setWriteToWAL(true);
-    this.internalPut(p, HConstants.DEFAULT_CLUSTER_ID, true);
+    doBatchMutate(p, lid);
   }
 
   /**
-   * Add updates first to the hlog (if writeToWal) and then add values to memstore.
-   * Warning: Assumption is caller has lock on passed in row.
-   * @param put The Put command
-   * @param clusterId UUID of the originating cluster (for replication).
-   * @param writeToWAL if true, then we should write to the log
-   * @throws IOException
-   */
-  private void internalPut(Put put, UUID clusterId, boolean writeToWAL) throws IOException {
-    Map<byte[], List<KeyValue>> familyMap = put.getFamilyMap();
-    WALEdit walEdit = new WALEdit();
-    /* run pre put hook outside of lock to avoid deadlock */
-    if (coprocessorHost != null) {
-      if (coprocessorHost.prePut(put, walEdit, writeToWAL)) {
-        return;
-      }
-    }
-
-    long now = EnvironmentEdgeManager.currentTimeMillis();
-    byte[] byteNow = Bytes.toBytes(now);
-    boolean flush = false;
-
-    lock(this.updatesLock.readLock());
-    try {
-      checkFamilies(familyMap.keySet());
-      checkTimestamps(familyMap, now);
-      updateKVTimestamps(familyMap.values(), byteNow);
-      // write/sync to WAL should happen before we touch memstore.
-      //
-      // If order is reversed, i.e. we write to memstore first, and
-      // for some reason fail to write/sync to commit log, the memstore
-      // will contain uncommitted transactions.
-      if (writeToWAL) {
-        addFamilyMapToWALEdit(familyMap, walEdit);
-        this.log.append(regionInfo, this.htableDescriptor.getName(),
-            walEdit, clusterId, now, this.htableDescriptor);
-      } else {
-        recordPutWithoutWal(familyMap);
-      }
-
-      long addedSize = applyFamilyMapToMemstore(familyMap, null);
-      flush = isFlushSize(this.addAndGetGlobalMemstoreSize(addedSize));
-    } finally {
-      this.updatesLock.readLock().unlock();
-    }
-
-    if (coprocessorHost != null) {
-      coprocessorHost.postPut(put, walEdit, writeToWAL);
-    }
-
-    // do after lock
-    final long after = EnvironmentEdgeManager.currentTimeMillis();
-    this.opMetrics.updatePutMetrics(familyMap.keySet(), after - now);
-
-    if (flush) {
-      // Request a cache flush.  Do it outside update lock.
-      requestFlush();
-    }
-  }
-
-  /**
    * Atomically apply the given map of family->edits to the memstore.
    * This handles the consistency control on its own, but the caller
    * should already have locked updatesLock.readLock(). This also does
@@ -4436,7 +4410,7 @@
       edits.add(new KeyValue(row, HConstants.CATALOG_FAMILY,
         HConstants.META_VERSION_QUALIFIER, now,
         Bytes.toBytes(HConstants.META_VERSION)));
-      meta.put(HConstants.CATALOG_FAMILY, edits);
+      meta.put(HConstants.CATALOG_FAMILY, edits, lid);
     } finally {
       meta.releaseRowLock(lid);
     }
@@ -4825,7 +4799,7 @@
 
   /**
    * Perform atomic mutations within the region.
-   * @param mutations The list of mutations to perform.
+   * @param mutationsList The list of mutations to perform.
    * <code>mutations</code> can contain operations for multiple rows.
    * Caller has to ensure that all rows are contained in this region.
    * @param rowsToLock Rows to lock
@@ -4833,7 +4807,7 @@
    * <code>rowsToLock</code> is sorted in order to avoid deadlocks.
    * @throws IOException
    */
-  public void mutateRowsWithLocks(Collection<Mutation> mutations,
+  public void mutateRowsWithLocks(Collection<Mutation> mutationsList,
       Collection<byte[]> rowsToLock) throws IOException {
     boolean flush = false;
 
@@ -4849,7 +4823,7 @@
       // one WALEdit is used for all edits.
       WALEdit walEdit = new WALEdit();
       if (coprocessorHost != null) {
-        for (Mutation m : mutations) {
+        for (Mutation m : mutationsList) {
           if (m instanceof Put) {
             if (coprocessorHost.prePut((Put) m, walEdit, m.getWriteToWAL())) {
               // by pass everything
@@ -4872,6 +4846,7 @@
 
       // 2. acquire the row lock(s)
       acquiredLocks = new ArrayList<Integer>(rowsToLock.size());
+      Map<byte[], Integer> locksMap = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
       for (byte[] row : rowsToLock) {
         // attempt to lock all involved rows, fail if one lock times out
         Integer lid = getLock(null, row, true);
@@ -4879,6 +4854,7 @@
           throw new IOException("Failed to acquire lock on "
               + Bytes.toStringBinary(row));
         }
+        locksMap.put(row, lid);
         acquiredLocks.add(lid);
       }
 
@@ -4889,12 +4865,45 @@
       // 4. Get a mvcc write number
       MultiVersionConsistencyControl.WriteEntry w = mvcc.beginMemstoreInsert();
 
+      BatchOperationInProgress<Pair<Mutation, Integer>> batchOp = null;
+      if (coprocessorHost != null) {
+        Pair<Mutation, Integer> putsAndLocks[] = new Pair[mutationsList.size()];
+        int count = 0;
+        for (Mutation m : mutationsList) {
+          Integer rowLock = locksMap.get(m.getRow());
+          putsAndLocks[count] = new Pair<Mutation, Integer>(m, rowLock);
+          ++count;
+        }
+        batchOp = new BatchOperationInProgress<Pair<Mutation, Integer>>(putsAndLocks);
+        List<Pair<Mutation, OperationStatus>> mutationVsBatchOp =
+            new ArrayList<Pair<Mutation, OperationStatus>>();
+        for (int i = 0; i < mutationsList.size(); i++) {
+          mutationVsBatchOp.add(new Pair<Mutation, OperationStatus>(batchOp.operations[i]
+              .getFirst(), batchOp.retCodeDetails[i]));
+        }
+        if (coprocessorHost.preBatchMutate(mutationVsBatchOp, walEdit)) {
+          return;
+        }
+        boolean fullBatchFailed = true;
+        for (int i = 0; i < mutationVsBatchOp.size(); i++) {
+          OperationStatus opStatus = mutationVsBatchOp.get(i).getSecond();
+          if (opStatus.getOperationStatusCode() != OperationStatusCode.NOT_RUN) {
+            batchOp.retCodeDetails[i] = mutationVsBatchOp.get(i).getSecond();
+          } else {
+            fullBatchFailed = false;
+          }
+        }
+        if (fullBatchFailed) {
+          return;
+        }
+      }
+
       long now = EnvironmentEdgeManager.currentTimeMillis();
       byte[] byteNow = Bytes.toBytes(now);
       Durability durability = Durability.USE_DEFAULT;
       try {
         // 5. Check mutations and apply edits to a single WALEdit
-        for (Mutation m : mutations) {
+        for (Mutation m : mutationsList) {
           if (m instanceof Put) {
             Map<byte[], List<KeyValue>> familyMap = m.getFamilyMap();
             checkFamilies(familyMap.keySet());
@@ -4918,6 +4927,12 @@
           }
         }
 
+        if (coprocessorHost != null) {
+          for (int i = 0; i < mutationsList.size(); i++) {
+            batchOp.retCodeDetails[i] = OperationStatus.SUCCESS;
+          }
+        }
+
         // 6. append all edits at once (don't sync)
         if (walEdit.size() > 0) {
           txid = this.log.appendNoSync(regionInfo,
@@ -4927,7 +4942,7 @@
 
         // 7. apply to memstore
         long addedSize = 0;
-        for (Mutation m : mutations) {
+        for (Mutation m : mutationsList) {
           addedSize += applyFamilyMapToMemstore(m.getFamilyMap(), w);
         }
         flush = isFlushSize(this.addAndGetGlobalMemstoreSize(addedSize));
@@ -4948,6 +4963,19 @@
         }
         walSyncSuccessful = true;
 
+        if (coprocessorHost != null) {
+          List<Mutation> mutations = new ArrayList<Mutation>();
+          for (int i = 0; i < mutationsList.size(); i++) {
+            // only for successful puts
+            if (batchOp.retCodeDetails[i].getOperationStatusCode() != OperationStatusCode.SUCCESS) {
+              continue;
+            }
+            Mutation m = batchOp.operations[i].getFirst();
+            mutations.add(m);
+          }
+          coprocessorHost.postBatchMutate(mutations, walEdit);
+        }
+
         // 10. advance mvcc
         mvcc.completeMemstoreInsert(w);
         w = null;
@@ -4956,7 +4984,7 @@
         // after the WAL is sync'ed and all locks are released
         // (similar to doMiniBatchPut)
         if (coprocessorHost != null) {
-          for (Mutation m : mutations) {
+          for (Mutation m : mutationsList) {
             if (m instanceof Put) {
               coprocessorHost.postPut((Put) m, walEdit, m.getWriteToWAL());
             } else if (m instanceof Delete) {
@@ -4968,7 +4996,7 @@
         // 12. clean up if needed
         if (!walSyncSuccessful) {
           int kvsRolledback = 0;
-          for (Mutation m : mutations) {
+          for (Mutation m : mutationsList) {
             for (Map.Entry<byte[], List<KeyValue>> e : m.getFamilyMap()
                 .entrySet()) {
               List<KeyValue> kvs = e.getValue();
@@ -4998,6 +5026,19 @@
             releaseRowLock(lid);
           }
         }
+
+        if (coprocessorHost != null) {
+          List<Mutation> mutations = new ArrayList<Mutation>();
+          for (int i = 0; i < mutationsList.size(); i++) {
+            // only for successful puts
+            if (batchOp.retCodeDetails[i].getOperationStatusCode() != OperationStatusCode.SUCCESS) {
+              continue;
+            }
+            Mutation m = batchOp.operations[i].getFirst();
+            mutations.add(m);
+          }
+          coprocessorHost.postCompleteBatchMutate(mutations);
+        }
       }
     } finally {
       if (flush) {
@@ -5655,7 +5696,7 @@
     return this.explicitSplitPoint;
   }
 
-  void forceSplit(byte[] sp) {
+  public void forceSplit(byte[] sp) {
     // NOTE : this HRegion will go away after the forced split is successfull
     //        therefore, no reason to clear this value
     this.splitRequest = true;
@@ -5761,10 +5802,13 @@
   }
 
   /**
-   * This method needs to be called before any public call that reads or
-   * modifies data. It has to be called just before a try.
-   * #closeRegionOperation needs to be called in the try's finally block
+   * This method needs to be called before any public call that reads or modifies data. It has to be
+   * called just before a try. #closeRegionOperation needs to be called in the try's finally block
    * Acquires a read lock and checks if the region is closing or closed.
+   * <p>
+   * Note: This method changed to public to support changes done in the IndexRegionObserver. Not
+   * advisable to used by users.
+   * <p>
    * @throws NotServingRegionException when the region is closing or closed
    * @throws RegionTooBusyException if failed to get the lock in time
    * @throws InterruptedIOException if interrupted while waiting for a lock
@@ -5784,8 +5828,12 @@
   }
 
   /**
-   * Closes the lock. This needs to be called in the finally block corresponding
-   * to the try block of #startRegionOperation
+   * Closes the lock. This needs to be called in the finally block corresponding to the try block of
+   * #startRegionOperation
+   * <p>
+   * Note: This method changed to public to support changes done in the IndexRegionObserver. Not
+   * advisable to used by users.
+   * <p>
    */
   public void closeRegionOperation(){
     lock.readLock().unlock();
@@ -5940,6 +5988,18 @@
     }
   };
 
+  public ReentrantReadWriteLock getUpdateLock() {
+    return this.updatesLock;
+  }
+
+  public void updateLock() {
+    this.updatesLock.readLock().lock();
+  }
+
+  public void releaseLock() {
+    this.updatesLock.readLock().unlock();
+  }
+
   /**
    * Facility for dumping and compacting catalog tables.
    * Only does catalog tables since these are only tables we for sure know
Index: src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java	(working copy)
@@ -53,10 +53,12 @@
 import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
 import org.apache.hadoop.hbase.coprocessor.RegionObserver;
+import org.apache.hadoop.hbase.coprocessor.RegionObserverExt;
 import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.filter.WritableByteArrayComparable;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
+import org.apache.hadoop.hbase.regionserver.SplitTransaction.SplitInfo;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
@@ -1565,7 +1567,86 @@
 
     return hasLoaded;
   }
-  
+
+  /**
+   * @param mutations
+   * @param edit
+   * @return
+   * @throws IOException
+   */
+  public boolean preBatchMutate(final List<Pair<Mutation, OperationStatus>> mutationVsBatchOp,
+      final WALEdit edit) throws IOException {
+    boolean bypass = false;
+    ObserverContext<RegionCoprocessorEnvironment> ctx = null;
+    for (RegionEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof RegionObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((RegionObserverExt) env.getInstance()).preBatchMutate(ctx, mutationVsBatchOp, edit);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        bypass |= ctx.shouldBypass();
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+    return bypass;
+  }
+
+  /**
+   * @param mutations
+   * @param walEdit
+   * @return
+   * @throws IOException
+   */
+  public boolean postBatchMutate(final List<Mutation> mutations, final WALEdit walEdit)
+      throws IOException {
+    boolean bypass = false;
+    ObserverContext<RegionCoprocessorEnvironment> ctx = null;
+    for (RegionEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof RegionObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((RegionObserverExt) env.getInstance()).postBatchMutate(ctx, mutations, walEdit);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        bypass |= ctx.shouldBypass();
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+    return bypass;
+  }
+
+  /**
+   * @param mutations
+   * @return
+   * @throws IOException
+   */
+  public boolean postCompleteBatchMutate(List<Mutation> mutations) throws IOException {
+    boolean bypass = false;
+    ObserverContext<RegionCoprocessorEnvironment> ctx = null;
+    for (RegionEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof RegionObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((RegionObserverExt) env.getInstance()).postCompleteBatchMutate(ctx, mutations);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        bypass |= ctx.shouldBypass();
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+    return bypass;
+  }
+
   public void preLockRow(byte[] regionName, byte[] row) throws IOException {
     ObserverContext<RegionCoprocessorEnvironment> ctx = null;
     for (RegionEnvironment env : coprocessors) {
@@ -1592,4 +1673,76 @@
     }
   }
 
+  public SplitInfo preSplitBeforePONR(byte[] splitKey) throws IOException {
+    SplitInfo info = null;
+    ObserverContext<RegionCoprocessorEnvironment> ctx = null;
+    for (RegionEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof RegionObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          info = ((RegionObserverExt) env.getInstance()).preSplitBeforePONR(ctx, splitKey);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+    return info;
+  }
+
+  public void preRollBack() throws IOException {
+    boolean result = false;
+    ObserverContext<RegionCoprocessorEnvironment> ctx = null;
+    for (RegionEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof RegionObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((RegionObserverExt) env.getInstance()).preRollBack(ctx);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  public void postCloseRegionOperation() throws IOException {
+    ObserverContext<RegionCoprocessorEnvironment> ctx = null;
+    for (RegionEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof RegionObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((RegionObserverExt) env.getInstance()).postCloseRegionOperation(ctx);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
+
+  // May be we can return boolean from here and use that boolean over there
+  // in HRegion before calling postCloseRegionOperation if there was no lock acquired
+  public void postStartRegionOperation() throws IOException {
+    ObserverContext<RegionCoprocessorEnvironment> ctx = null;
+    for (RegionEnvironment env : coprocessors) {
+      if (env.getInstance() instanceof RegionObserverExt) {
+        ctx = ObserverContext.createAndPrepare(env, ctx);
+        try {
+          ((RegionObserverExt) env.getInstance()).postStartRegionOperation(ctx);
+        } catch (Throwable e) {
+          handleCoprocessorThrowable(env, e);
+        }
+        if (ctx.shouldComplete()) {
+          break;
+        }
+      }
+    }
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java	(working copy)
@@ -37,11 +37,16 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HBaseFileSystem;
 import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.executor.EventHandler.EventType;
 import org.apache.hadoop.hbase.executor.RegionTransitionData;
 import org.apache.hadoop.hbase.io.Reference.Range;
@@ -145,6 +150,8 @@
    */
   private final List<JournalEntry> journal = new ArrayList<JournalEntry>();
 
+  static final String INDEX_TABLE_SUFFIX = "_idx";
+
   /**
    * Constructor
    * @param r Region to split
@@ -215,6 +222,10 @@
   /* package */PairOfSameType<HRegion> createDaughters(final Server server,
       final RegionServerServices services) throws IOException {
     LOG.info("Starting split of region " + this.parent);
+    boolean secondaryIndex =
+        server == null ? false : server.getConfiguration().getBoolean("hbase.use.secondary.index",
+          false);
+    boolean indexRegionAvailable = false;
     if ((server != null && server.isStopped()) ||
         (services != null && services.isStopping())) {
       throw new IOException("Server is stopped or stopping");
@@ -226,13 +237,107 @@
       this.parent.getCoprocessorHost().preSplit();
     }
 
-    // If true, no cluster to write meta edits to or to update znodes in.
     boolean testing = server == null? true:
       server.getConfiguration().getBoolean("hbase.testing.nocluster", false);
-    this.fileSplitTimeout = testing ? this.fileSplitTimeout :
-      server.getConfiguration().getLong("hbase.regionserver.fileSplitTimeout",
-          this.fileSplitTimeout);
 
+    PairOfSameType<HRegion> daughterRegionsPair = stepsBeforeAddingPONR(server, services, testing);
+
+    SplitInfo info = null;
+    // Coprocessor callback
+    if (secondaryIndex) {
+      if (this.parent.getCoprocessorHost() != null) {
+        info = this.parent.getCoprocessorHost().preSplitBeforePONR(this.splitrow);
+        if (info == null) {
+          throw new IOException("Pre split of Index region has failed.");
+        }
+        if ((info.getSplitTransaction() != null && info.getDaughters() != null)) {
+          indexRegionAvailable = true;
+        }
+      }
+    }
+
+    // add one hook
+    // do the step till started_region_b_creation
+    // This is the point of no return.  Adding subsequent edits to .META. as we
+    // do below when we do the daughter opens adding each to .META. can fail in
+    // various interesting ways the most interesting of which is a timeout
+    // BUT the edits all go through (See HBASE-3872).  IF we reach the PONR
+    // then subsequent failures need to crash out this regionserver; the
+    // server shutdown processing should be able to fix-up the incomplete split.
+    // The offlined parent will have the daughters as extra columns.  If
+    // we leave the daughter regions in place and do not remove them when we
+    // crash out, then they will have their references to the parent in place
+    // still and the server shutdown fixup of .META. will point to these
+    // regions.
+    // We should add PONR JournalEntry before offlineParentInMeta,so even if
+    // OfflineParentInMeta timeout,this will cause regionserver exit,and then
+    // master ServerShutdownHandler will fix daughter & avoid data loss. (See
+    // HBase-4562).
+    this.journal.add(JournalEntry.PONR);
+
+    // Edit parent in meta.  Offlines parent region and adds splita and splitb.
+    if (!testing) {
+      if (!indexRegionAvailable) {
+        MetaEditor.offlineParentInMeta(server.getCatalogTracker(), this.parent.getRegionInfo(),
+          daughterRegionsPair.getFirst().getRegionInfo(), daughterRegionsPair.getSecond()
+              .getRegionInfo());
+      } else {
+        offlineParentInMetaBothIndexAndMainRegion(server.getCatalogTracker(),
+          this.parent.getRegionInfo(), daughterRegionsPair.getFirst().getRegionInfo(),
+          daughterRegionsPair.getSecond().getRegionInfo(),
+          info.getSplitTransaction().parent.getRegionInfo(), info.getDaughters().getFirst()
+              .getRegionInfo(), info.getDaughters().getSecond().getRegionInfo());
+      }
+    }
+    return daughterRegionsPair;
+  }
+
+  private static void offlineParentInMetaBothIndexAndMainRegion(CatalogTracker catalogTracker,
+      HRegionInfo parent, final HRegionInfo a, final HRegionInfo b, final HRegionInfo parentIdx,
+      final HRegionInfo idxa, final HRegionInfo idxb) throws NotAllMetaRegionsOnlineException,
+      IOException {
+    HRegionInfo copyOfParent = new HRegionInfo(parent);
+    copyOfParent.setOffline(true);
+    copyOfParent.setSplit(true);
+    List<Put> list = new ArrayList<Put>();
+    Put put = new Put(copyOfParent.getRegionName());
+    put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
+      Writables.getBytes(copyOfParent));
+    put.add(HConstants.CATALOG_FAMILY, HConstants.SPLITA_QUALIFIER, Writables.getBytes(a));
+    put.add(HConstants.CATALOG_FAMILY, HConstants.SPLITB_QUALIFIER, Writables.getBytes(b));
+    list.add(put);
+
+    HRegionInfo copyOfIdxParent = new HRegionInfo(parentIdx);
+    copyOfIdxParent.setOffline(true);
+    copyOfIdxParent.setSplit(true);
+    Put putForIdxRegion = new Put(copyOfIdxParent.getRegionName());
+    putForIdxRegion.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
+      Writables.getBytes(copyOfIdxParent));
+    putForIdxRegion.add(HConstants.CATALOG_FAMILY, HConstants.SPLITA_QUALIFIER,
+      Writables.getBytes(idxa));
+    putForIdxRegion.add(HConstants.CATALOG_FAMILY, HConstants.SPLITB_QUALIFIER,
+      Writables.getBytes(idxb));
+    list.add(putForIdxRegion);
+    putToMetaTable(catalogTracker, list);
+    LOG.info("Offlined parent region " + parent.getRegionNameAsString() + " in META");
+  }
+
+  private static void putToMetaTable(final CatalogTracker ct, final List<Put> p) throws IOException {
+    org.apache.hadoop.hbase.client.HConnection c = ct.getConnection();
+    if (c == null) throw new NullPointerException("No connection");
+    put(new HTable(ct.getConnection().getConfiguration(), HConstants.META_TABLE_NAME), p);
+  }
+
+  private static void put(final HTable t, final List<Put> p) throws IOException {
+    try {
+      t.put(p);
+    } finally {
+      t.close();
+    }
+  }
+
+  public PairOfSameType<HRegion> stepsBeforeAddingPONR(final Server server, final RegionServerServices services,
+      boolean testing) throws IOException {
     // Set ephemeral SPLITTING znode up in zk.  Mocked servers sometimes don't
     // have zookeeper so don't do zk stuff if server or zookeeper is null
     if (server != null && server.getZooKeeper() != null) {
@@ -307,30 +412,7 @@
     // Ditto
     this.journal.add(JournalEntry.STARTED_REGION_B_CREATION);
     HRegion b = createDaughterRegion(this.hri_b, this.parent.rsServices);
-
-    // This is the point of no return.  Adding subsequent edits to .META. as we
-    // do below when we do the daughter opens adding each to .META. can fail in
-    // various interesting ways the most interesting of which is a timeout
-    // BUT the edits all go through (See HBASE-3872).  IF we reach the PONR
-    // then subsequent failures need to crash out this regionserver; the
-    // server shutdown processing should be able to fix-up the incomplete split.
-    // The offlined parent will have the daughters as extra columns.  If
-    // we leave the daughter regions in place and do not remove them when we
-    // crash out, then they will have their references to the parent in place
-    // still and the server shutdown fixup of .META. will point to these
-    // regions.
-    // We should add PONR JournalEntry before offlineParentInMeta,so even if
-    // OfflineParentInMeta timeout,this will cause regionserver exit,and then
-    // master ServerShutdownHandler will fix daughter & avoid data loss. (See 
-    // HBase-4562).
-    this.journal.add(JournalEntry.PONR);
-
-    // Edit parent in meta.  Offlines parent region and adds splita and splitb.
-    if (!testing) {
-      MetaEditor.offlineParentInMeta(server.getCatalogTracker(),
-        this.parent.getRegionInfo(), a.getRegionInfo(), b.getRegionInfo());
-    }
-    return new PairOfSameType<HRegion>(a, b);
+    return new PairOfSameType<HRegion>(a,b);
   }
 
   /**
@@ -458,9 +540,14 @@
       final RegionServerServices services)
   throws IOException {
     PairOfSameType<HRegion> regions = createDaughters(server, services);
+    stepsAfterPONR(server, services, regions);
+    return regions;
+  }
+
+  public void stepsAfterPONR(final Server server, final RegionServerServices services,
+      PairOfSameType<HRegion> regions) throws IOException {
     openDaughters(server, services, regions.getFirst(), regions.getSecond());
     transitionZKNode(server, services, regions.getFirst(), regions.getSecond());
-    return regions;
   }
 
   /*
@@ -584,8 +671,8 @@
     }
   }
 
-  private void splitStoreFiles(final Path splitdir,
-    final List<StoreFile> hstoreFilesToSplit)
+  protected void splitStoreFiles(final Path splitdir,
+      final List<StoreFile> hstoreFilesToSplit)
   throws IOException {
     if (hstoreFilesToSplit == null) {
       // Could be null because close didn't succeed -- for now consider it fatal
@@ -595,10 +682,16 @@
     // there's files to split. It then fires up everything, waits for
     // completion and finally checks for any exception
     int nbFiles = hstoreFilesToSplit.size();
-    if (nbFiles == 0) {
-      // no file needs to be splitted.
-      return;
+    boolean secondaryIndex = this.parent.getConf().getBoolean("hbase.use.secondary.index", false);
+    if (secondaryIndex) {
+      String idxTableName = this.parent.getTableDesc().getNameAsString();
+      if (isIndexTable(idxTableName)) if (nbFiles == 0) {
+        LOG.debug("Setting number of threads for ThreadPoolExecutor to 1 since IndexTable "
+            + idxTableName + " doesn't have any store files ");
+        nbFiles = 1;
+      }
     }
+
     ThreadFactoryBuilder builder = new ThreadFactoryBuilder();
     builder.setNameFormat("StoreFileSplitter-%1$d");
     ThreadFactory factory = builder.build();
@@ -647,6 +740,10 @@
     }
   }
 
+  private boolean isIndexTable(String idxTableName) {
+    return idxTableName.endsWith(INDEX_TABLE_SUFFIX);
+  }
+
   private void splitStoreFile(final StoreFile sf, final Path splitdir)
   throws IOException {
     FileSystem fs = this.parent.getFilesystem();
@@ -743,6 +840,15 @@
    */
   public boolean rollback(final Server server, final RegionServerServices services)
   throws IOException {
+    // Coprocessor callback
+    boolean secondaryIndex =
+        server == null ? false : server.getConfiguration().getBoolean("hbase.use.secondary.index",
+          false);
+    if (secondaryIndex) {
+      if (this.parent.getCoprocessorHost() != null) {
+        this.parent.getCoprocessorHost().preRollBack();
+      }
+    }
     boolean result = true;
     FileSystem fs = this.parent.getFilesystem();
     ListIterator<JournalEntry> iterator =
@@ -928,8 +1034,35 @@
       znodeVersion, payload);
   }
 
+  public static class SplitInfo {
+    PairOfSameType<HRegion> pairOfSameType;
+    SplitTransaction st;
+
+    public void setDaughtersAndTransaction(PairOfSameType<HRegion> pairOfSameType,
+        SplitTransaction st) {
+      this.pairOfSameType = pairOfSameType;
+      this.st = st;
+    }
+
+    public PairOfSameType<HRegion> getDaughters() {
+      return this.pairOfSameType;
+    }
+
+    public SplitTransaction getSplitTransaction() {
+      return this.st;
+    }
+  }
+
   /**
-   * 
+   * Added for secondary index. Needed in the hooks to get the parentRegion name
+   * @return
+   */
+  public HRegion getParent() {
+    return this.parent;
+  }
+
+  /**
+   *
    * @param zkw zk reference
    * @param parent region to be transitioned to splitting
    * @param serverName server event originates from
Index: src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java	(working copy)
@@ -22,6 +22,7 @@
 import java.io.DataInput;
 import java.io.FileNotFoundException;
 import java.io.IOException;
+import java.lang.reflect.Constructor;
 import java.nio.ByteBuffer;
 import java.util.Arrays;
 import java.util.Collection;
@@ -61,8 +62,8 @@
 import org.apache.hadoop.hbase.io.hfile.HFileWriterV1;
 import org.apache.hadoop.hbase.io.hfile.HFileWriterV2;
 import org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder;
+import org.apache.hadoop.hbase.regionserver.metrics.SchemaConfigured;
 import org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics;
-import org.apache.hadoop.hbase.regionserver.metrics.SchemaConfigured;
 import org.apache.hadoop.hbase.util.BloomFilter;
 import org.apache.hadoop.hbase.util.BloomFilterFactory;
 import org.apache.hadoop.hbase.util.BloomFilterWriter;
@@ -142,6 +143,8 @@
   // Make default block size for StoreFiles 8k while testing.  TODO: FIX!
   // Need to make it 8k for testing.
   public static final int DEFAULT_BLOCKSIZE_SMALL = 8 * 1024;
+  
+  private static boolean useIndex;
 
   private final FileSystem fs;
 
@@ -160,6 +163,8 @@
   // Block cache configuration and reference.
   private final CacheConfig cacheConf;
 
+  private Configuration conf;
+
   // What kind of data block encoding will be used
   private final HFileDataBlockEncoder dataBlockEncoder;
 
@@ -261,6 +266,8 @@
     this.fs = fs;
     this.path = p;
     this.cacheConf = cacheConf;
+    this.conf = conf;
+    useIndex = conf.getBoolean("hbase.use.secondary.index", false);
     this.dataBlockEncoder =
         dataBlockEncoder == null ? NoOpDataBlockEncoder.INSTANCE
             : dataBlockEncoder;
@@ -537,8 +544,31 @@
         this.reader = new HalfStoreFileReader(this.fs, this.referencePath, this.link,
           this.cacheConf, this.reference, dataBlockEncoder.getEncodingInCache());
       } else {
-        this.reader = new HalfStoreFileReader(this.fs, this.referencePath,
-          this.cacheConf, this.reference, dataBlockEncoder.getEncodingInCache());
+        if (conf.getBoolean("hbase.use.secondary.index", false) && isIndexRegionReference()) {
+          String indexHalfStoreFileClass = conf.get("hbase.index.half.storefile.reader.class");
+          if (indexHalfStoreFileClass == null) {
+            throw new RuntimeException(
+                "Class for index half store files should be present.  Configure the property hbase.index.half.storefile.reader.class");
+          }
+          try {
+            Class<?> indexHalfStoreReader = Class.forName(indexHalfStoreFileClass.trim());
+            Constructor<?> constructor =
+                indexHalfStoreReader.getConstructor(FileSystem.class, Path.class,
+                  CacheConfig.class, Reference.class, DataBlockEncoding.class);
+            this.reader =
+                (Reader) constructor.newInstance(this.fs, this.referencePath, this.cacheConf,
+                  this.reference, dataBlockEncoder.getEncodingInCache());
+          } catch (Throwable e) {
+            LOG.error("Error while initializing/invoking constructor of IndexHalfStoreFileReader.",
+              e);
+            throw new RuntimeException(
+                "Error while initializing/invoking constructor of IndexHalfStoreFileReader.");
+          }
+        } else {
+          this.reader =
+              new HalfStoreFileReader(this.fs, this.referencePath, this.cacheConf, this.reference,
+                  dataBlockEncoder.getEncodingInCache());
+        }
       }
     } else if (isLink()) {
       long size = link.getFileStatus(fs).getLen();
@@ -629,6 +659,11 @@
     return this.reader;
   }
 
+  private boolean isIndexRegionReference() {
+    String tablePath = this.referencePath.getParent().getParent().getParent().getName();
+    return tablePath.endsWith("_idx");
+  }
+
   /**
    * @return Reader for StoreFile. creates if necessary
    * @throws IOException
@@ -949,24 +984,27 @@
 	    
     // Check whether the split row lies in the range of the store file
     // If it is outside the range, return directly.
-    if (range == Reference.Range.bottom) {
-      //check if smaller than first key
-      KeyValue splitKey = KeyValue.createLastOnRow(splitRow);
-      byte[] firstKey = f.createReader().getFirstKey();
-      if (f.getReader().getComparator().compare(splitKey.getBuffer(), 
-          splitKey.getKeyOffset(), splitKey.getKeyLength(), 
+    if (!useIndex || !isIndexTable(f.getPath())) {
+      if (range == Reference.Range.bottom) {
+        //check if smaller than first key
+        KeyValue splitKey = KeyValue.createLastOnRow(splitRow);
+        byte[] firstKey = f.createReader().getFirstKey();
+        if (firstKey == null) return null;
+        if (f.getReader().getComparator().compare(splitKey.getBuffer(),
+          splitKey.getKeyOffset(), splitKey.getKeyLength(),
           firstKey, 0, firstKey.length) < 0) {
         return null;
       }      
-    }
-    else {
+    } else {
       //check if larger than last key.
-      KeyValue splitKey = KeyValue.createFirstOnRow(splitRow);
-      byte[] lastKey = f.createReader().getLastKey();      
-      if (f.getReader().getComparator().compare(splitKey.getBuffer(), 
+        KeyValue splitKey = KeyValue.createFirstOnRow(splitRow);
+        byte[] lastKey = f.createReader().getLastKey();
+        if (lastKey == null) return null;
+        if (f.getReader().getComparator().compare(splitKey.getBuffer(), 
           splitKey.getKeyOffset(), splitKey.getKeyLength(), 
           lastKey, 0, lastKey.length) > 0) {
-        return null;
+          return null;
+        }
       }
     }
     
@@ -984,6 +1022,11 @@
   }
 
 
+  private static boolean isIndexTable(Path path) {
+    String tablePath = path.getParent().getParent().getParent().getName();
+    return tablePath.endsWith("_idx");
+  }
+
   /**
    * A StoreFile writer.  Use this to read/write HBase Store Files. It is package
    * local because it is an implementation detail of the HBase regionserver.
Index: src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java	(working copy)
@@ -92,6 +92,7 @@
 import org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler;
 import org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl;
 import org.apache.hadoop.hbase.zookeeper.RootRegionTracker;
+import org.apache.hadoop.hbase.zookeeper.ZKTable;
 import org.apache.hadoop.hbase.zookeeper.ZKTableReadOnly;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
 import org.apache.hadoop.security.AccessControlException;
@@ -100,6 +101,7 @@
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.zookeeper.KeeperException;
+import org.apache.hadoop.hbase.zookeeper.ZKTable.TableState;
 
 import com.google.common.base.Joiner;
 import com.google.common.base.Preconditions;
@@ -1232,8 +1234,11 @@
       public Void connect(HConnection connection) throws IOException {
         ZooKeeperWatcher zkw = connection.getZooKeeperWatcher();
         try {
-          for (String tableName : ZKTableReadOnly.getDisabledOrDisablingTables(zkw)) {
-            disabledTables.add(Bytes.toBytes(tableName));
+          for (Entry<TableState, Set<String>> e : ZKTableReadOnly.getDisabledOrDisablingTables(zkw)
+              .entrySet()) {
+            for (String tableName : e.getValue()) {
+              disabledTables.add(Bytes.toBytes(tableName));
+            }
           }
         } catch (KeeperException ke) {
           throw new IOException(ke);
Index: src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableReadOnly.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableReadOnly.java	(revision 1505907)
+++ src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableReadOnly.java	(working copy)
@@ -19,9 +19,11 @@
  */
 package org.apache.hadoop.hbase.zookeeper;
 
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
+import java.util.Map;
 
 import org.apache.hadoop.hbase.master.AssignmentManager;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -105,22 +107,70 @@
 
   /**
    * Gets a list of all the tables set as disabled in zookeeper.
-   * @return Set of disabled tables, empty Set if none
+   * @return Map of disabled tables, empty MAP if none
    * @throws KeeperException
    */
-  public static Set<String> getDisabledOrDisablingTables(ZooKeeperWatcher zkw)
-  throws KeeperException {
-    Set<String> disabledTables = new HashSet<String>();
-    List<String> children =
-      ZKUtil.listChildrenNoWatch(zkw, zkw.clientTableZNode);
-    for (String child: children) {
+  public static Map<TableState, Set<String>> getDisabledOrDisablingTables(ZooKeeperWatcher zkw)
+      throws KeeperException {
+    Map<TableState, Set<String>> disabledTables = new HashMap<TableState, Set<String>>();
+    List<String> children = ZKUtil.listChildrenNoWatch(zkw, zkw.clientTableZNode);
+    for (String child : children) {
       TableState state = getTableState(zkw, child);
-      if (state == TableState.DISABLED || state == TableState.DISABLING)
-        disabledTables.add(child);
+      if (state == TableState.DISABLED) {
+        Set<String> tables = disabledTables.get(state);
+        if (tables == null) {
+          tables = new HashSet<String>();
+          disabledTables.put(TableState.DISABLED, tables);
+        }
+        tables.add(child);
+        continue;
+      }
+      if (state == TableState.DISABLING) {
+        Set<String> tables = disabledTables.get(state);
+        if (tables == null) {
+          tables = new HashSet<String>();
+          disabledTables.put(TableState.DISABLING, tables);
+        }
+        tables.add(child);
+        continue;
+      }
     }
     return disabledTables;
   }
 
+  /**
+   * Gets a list of all the tables set as enabled or enabling in zookeeper.
+   * @return Map of enabled tables, empty map if none
+   * @throws KeeperException
+   */
+  public static Map<TableState, Set<String>> getEnabledOrEnablingTables(ZooKeeperWatcher zkw)
+      throws KeeperException {
+    Map<TableState, Set<String>> enabledTables = new HashMap<TableState, Set<String>>();
+    List<String> children = ZKUtil.listChildrenNoWatch(zkw, zkw.clientTableZNode);
+    for (String child : children) {
+      TableState state = getTableState(zkw, child);
+      if (state == TableState.ENABLED) {
+        Set<String> tables = enabledTables.get(state);
+        if (tables == null) {
+          tables = new HashSet<String>();
+          enabledTables.put(TableState.ENABLED, tables);
+        }
+        tables.add(child);
+        continue;
+      }
+      if (state == TableState.ENABLING) {
+        Set<String> tables = enabledTables.get(state);
+        if (tables == null) {
+          tables = new HashSet<String>();
+          enabledTables.put(TableState.ENABLING, tables);
+        }
+        tables.add(child);
+        continue;
+      }
+    }
+    return enabledTables;
+  }
+
   static boolean isTableState(final TableState expectedState,
     final TableState currentState) {
     return currentState != null && currentState.equals(expectedState);
Index: src/main/resources/hbase-default.xml
===================================================================
--- src/main/resources/hbase-default.xml	(revision 1505907)
+++ src/main/resources/hbase-default.xml	(working copy)
@@ -907,6 +907,29 @@
 	In both cases, the aggregated metric M across tables and cfs will be reported.
     </description>
   </property>
+ <property>
+	<name>hbase.use.secondary.index</name>
+	<value>false</value>
+	<description>Enable this property when you are using secondary index.
+	</description>
+ </property>
+ <property>
+	<name>hbase.index.half.storefile.reader.class</name>
+	<value>org.apache.hadoop.hbase.index.io.IndexHalfStoreFileReader
+	</value>
+	<description>Do not change this. It is used internally when secondary
+		index is enabled
+	</description>
+ </property>
+ <property>
+	<name>hbase.index.loadbalancer.class</name>
+	<value>
+		org.apache.hadoop.hbase.index.SecIndexLoadBalancer
+	</value>
+	<description>Do not change this. It is used internally when secondary
+		index is enabled.
+	</description>
+ </property>
   <property>
     <name>hbase.table.archive.directory</name>
     <value>.archive</value>
Index: src/main/ruby/hbase/admin.rb
===================================================================
--- src/main/ruby/hbase/admin.rb	(revision 1505907)
+++ src/main/ruby/hbase/admin.rb	(working copy)
@@ -308,19 +308,56 @@
     #----------------------------------------------------------------------------------------------
     # Truncates table (deletes all records by recreating the table)
     def truncate(table_name, conf = @conf)
+      raise ArgumentError, "Truncate on index table is not supported." unless !(table_name.end_with?("_idx"))
+      raise ArgumentError, "Table #{table_name} is not enabled. Enable it first.'" unless enabled?(table_name)
+      
       h_table = org.apache.hadoop.hbase.client.HTable.new(conf, table_name)
-      table_description = h_table.getTableDescriptor()
+      is_index_table_exist = @admin.tableExists(org.apache.hadoop.hbase.index.util.IndexUtils.getIndexTableName(table_name))
+
+      if is_index_table_exist	
+		table_description = org.apache.hadoop.hbase.index.util.IndexUtils.readIndexedHTableDescriptor(table_name, conf)	       
+      else
+        table_description = h_table.getTableDescriptor()
+      end 
+
+      yield 'Disabling table...' if block_given?
+      @admin.disableTable(table_name)
+
+      yield 'Dropping table...' if block_given?
+      @admin.deleteTable(table_name)
+     
+	  yield 'Creating table...' if block_given?
+	  @admin.createTable(table_description)
+    end
+    
+    #----------------------------------------------------------------------------------------------
+    # Truncates table while maintaing region boundaries (deletes all records by recreating the table)
+    def truncate_preserve(table_name, conf = @conf)
+      raise ArgumentError, "Truncate on index table is not supported." unless !(table_name.end_with?("_idx"))
       raise ArgumentError, "Table #{table_name} is not enabled. Enable it first.'" unless enabled?(table_name)
+       
+      h_table = org.apache.hadoop.hbase.client.HTable.new(conf, table_name)
+      splitKeys = h_table.getSplitKeys()
+      is_index_table_exist = @admin.tableExists(org.apache.hadoop.hbase.index.util.IndexUtils.getIndexTableName(table_name))
+
+      if is_index_table_exist	
+		table_description = org.apache.hadoop.hbase.index.util.IndexUtils.readIndexedHTableDescriptor(table_name, conf)
+	  else
+	  	table_description = h_table.getTableDescriptor()
+      end
+      
       yield 'Disabling table...' if block_given?
       @admin.disableTable(table_name)
 
       yield 'Dropping table...' if block_given?
       @admin.deleteTable(table_name)
-
-      yield 'Creating table...' if block_given?
-      @admin.createTable(table_description)
+     
+	  yield 'Creating table with region boundaries...' if block_given?
+	  @admin.createTable(table_description, splitKeys)     
     end
 
+    #----------------------------------------------------------------------------------------------
+
     # Check the status of alter command (number of regions reopened)
     def alter_status(table_name)
       # Table name should be a string
Index: src/main/ruby/shell.rb
===================================================================
--- src/main/ruby/shell.rb	(revision 1505907)
+++ src/main/ruby/shell.rb	(working copy)
@@ -251,6 +251,7 @@
     put
     scan
     truncate
+    truncate_preserve
   ]
 )
 
Index: src/main/ruby/shell/commands/truncate_preserve.rb
===================================================================
--- src/main/ruby/shell/commands/truncate_preserve.rb	(revision 0)
+++ src/main/ruby/shell/commands/truncate_preserve.rb	(revision 0)
@@ -0,0 +1,38 @@
+#
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class TruncatePreserve < Command
+      def help
+        return <<-EOF
+  Disables, drops and recreates the specified table while still maintaing the previous region boundaries.
+EOF
+      end
+
+      def command(table)
+        format_simple_command do
+          puts "Truncating '#{table}' table (it may take a while):"
+          admin.truncate_preserve(table) { |log| puts " - #{log}" }
+        end
+      end
+
+    end
+  end
+end
\ No newline at end of file
Index: src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java	(revision 1505907)
+++ src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java	(working copy)
@@ -48,8 +48,10 @@
 import org.apache.hadoop.hbase.regionserver.KeyValueScanner;
 import org.apache.hadoop.hbase.regionserver.Leases;
 import org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress;
+import org.apache.hadoop.hbase.regionserver.OperationStatus;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.regionserver.ScanType;
+import org.apache.hadoop.hbase.regionserver.SplitTransaction.SplitInfo;
 import org.apache.hadoop.hbase.regionserver.Store;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
@@ -60,7 +62,7 @@
  * A sample region observer that tests the RegionObserver interface.
  * It works with TestRegionObserverInterface to provide the test case.
  */
-public class SimpleRegionObserver extends BaseRegionObserver {
+public class SimpleRegionObserver extends BaseRegionObserver implements RegionObserverExt {
   static final Log LOG = LogFactory.getLog(TestRegionObserverInterface.class);
 
   boolean beforeDelete = true;
@@ -555,4 +557,65 @@
   public boolean hadPreBulkLoadHFile() {
     return hadPreBulkLoadHFile;
   }
+
+  @Override
+  public void preBatchMutate(ObserverContext<RegionCoprocessorEnvironment> ctx,
+      List<Pair<Mutation, OperationStatus>> mutationVsBatchOp, WALEdit edit) throws IOException {
+    RegionCoprocessorEnvironment e = ctx.getEnvironment();
+    assertNotNull(e);
+    assertNotNull(e.getRegion());
+    assertNotNull(edit);
+    hadPreBatchMutate = true;
+  }
+
+  @Override
+  public void postBatchMutate(ObserverContext<RegionCoprocessorEnvironment> ctx,
+      List<Mutation> mutations, WALEdit walEdit) throws IOException {
+    RegionCoprocessorEnvironment e = ctx.getEnvironment();
+    assertNotNull(e);
+    assertNotNull(e.getRegion());
+    assertNotNull(walEdit);
+    hadPostBatchMutate = true;
+  }
+
+  @Override
+  public void postCompleteBatchMutate(ObserverContext<RegionCoprocessorEnvironment> ctx,
+      List<Mutation> mutations) throws IOException {
+    // TODO Auto-generated method stub
+    
+  }
+
+  @Override
+  public boolean postFilterRow(ObserverContext<RegionCoprocessorEnvironment> ctx,
+      InternalScanner s, byte[] currentRow) throws IOException {
+    // TODO Auto-generated method stub
+    return false;
+  }
+
+  @Override
+  public SplitInfo preSplitBeforePONR(ObserverContext<RegionCoprocessorEnvironment> ctx,
+      byte[] splitKey) throws IOException {
+    // TODO Auto-generated method stub
+    return null;
+  }
+
+  @Override
+  public void preRollBack(ObserverContext<RegionCoprocessorEnvironment> ctx) throws IOException {
+    // TODO Auto-generated method stub
+    
+  }
+
+  @Override
+  public void postCloseRegionOperation(ObserverContext<RegionCoprocessorEnvironment> ctx)
+      throws IOException {
+    // TODO Auto-generated method stub
+    
+  }
+
+  @Override
+  public void postStartRegionOperation(ObserverContext<RegionCoprocessorEnvironment> ctx)
+      throws IOException {
+    // TODO Auto-generated method stub
+    
+  }
 }
Index: src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java	(revision 1505907)
+++ src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java	(working copy)
@@ -30,6 +30,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableMap;
+import java.util.concurrent.CountDownLatch;
 
 import junit.framework.Assert;
 
@@ -60,6 +61,8 @@
 public class TestMasterObserver {
   private static final Log LOG = LogFactory.getLog(TestMasterObserver.class);
 
+  public static CountDownLatch countDown = new CountDownLatch(1);
+
   public static class CPMasterObserver implements MasterObserver {
 
     private boolean bypass = false;
@@ -94,6 +97,22 @@
     private boolean postStartMasterCalled;
     private boolean startCalled;
     private boolean stopCalled;
+    private boolean preCreateTableHandlerCalled;
+    private boolean postCreateTableHandlerCalled;
+    private boolean preDeleteTableHandlerCalled;
+    private boolean postDeleteTableHandlerCalled;
+    private boolean preAddColumnHandlerCalled;
+    private boolean postAddColumnHandlerCalled;
+    private boolean preModifyColumnHandlerCalled;
+    private boolean postModifyColumnHandlerCalled;
+    private boolean preDeleteColumnHandlerCalled;
+    private boolean postDeleteColumnHandlerCalled;
+    private boolean preEnableTableHandlerCalled;
+    private boolean postEnableTableHandlerCalled;
+    private boolean preDisableTableHandlerCalled;
+    private boolean postDisableTableHandlerCalled;
+    private boolean preModifyTableHandlerCalled;
+    private boolean postModifyTableHandlerCalled;
     private boolean preSnapshotCalled;
     private boolean postSnapshotCalled;
     private boolean preCloneSnapshotCalled;
@@ -134,6 +153,22 @@
       postBalanceCalled = false;
       preBalanceSwitchCalled = false;
       postBalanceSwitchCalled = false;
+      preCreateTableHandlerCalled = false;
+      postCreateTableHandlerCalled = false;
+      preDeleteTableHandlerCalled = false;
+      postDeleteTableHandlerCalled = false;
+      preModifyTableHandlerCalled = false;
+      postModifyTableHandlerCalled = false;
+      preAddColumnHandlerCalled = false;
+      postAddColumnHandlerCalled = false;
+      preModifyColumnHandlerCalled = false;
+      postModifyColumnHandlerCalled = false;
+      preDeleteColumnHandlerCalled = false;
+      postDeleteColumnHandlerCalled = false;
+      preEnableTableHandlerCalled = false;
+      postEnableTableHandlerCalled = false;
+      preDisableTableHandlerCalled = false;
+      postDisableTableHandlerCalled = false;
       preSnapshotCalled = false;
       postSnapshotCalled = false;
       preCloneSnapshotCalled = false;
@@ -483,6 +518,195 @@
     public boolean wasStopped() { return stopCalled; }
 
     @Override
+    public void preCreateTableHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preCreateTableHandlerCalled = true;
+    }
+
+    @Override
+    public void postCreateTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+        HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+      postCreateTableHandlerCalled = true;
+      countDown.countDown();
+    }
+
+    public boolean wasPreCreateTableHandlerCalled() {
+      return preCreateTableHandlerCalled;
+    }
+
+    public boolean wasCreateTableHandlerCalled() {
+      return preCreateTableHandlerCalled && postCreateTableHandlerCalled;
+    }
+
+    public boolean wasCreateTableHandlerCalledOnly() {
+      return preCreateTableHandlerCalled && !postCreateTableHandlerCalled;
+    }
+
+    @Override
+    public void preDeleteTableHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preDeleteTableHandlerCalled = true;
+    }
+
+    @Override
+    public void postDeleteTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+        byte[] tableName) throws IOException {
+      postDeleteTableHandlerCalled = true;
+    }
+
+    public boolean wasDeleteTableHandlerCalled() {
+      return preDeleteTableHandlerCalled && postDeleteTableHandlerCalled;
+    }
+
+    public boolean wasDeleteTableHandlerCalledOnly() {
+      return preDeleteTableHandlerCalled && !postDeleteTableHandlerCalled;
+    }
+
+    @Override
+    public void preModifyTableHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName, HTableDescriptor htd) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preModifyTableHandlerCalled = true;
+    }
+
+    @Override
+    public void postModifyTableHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName, HTableDescriptor htd) throws IOException {
+      postModifyTableHandlerCalled = true;
+    }
+
+    public boolean wasModifyTableHandlerCalled() {
+      return preModifyColumnHandlerCalled && postModifyColumnHandlerCalled;
+    }
+
+    public boolean wasModifyTableHandlerCalledOnly() {
+      return preModifyColumnHandlerCalled && !postModifyColumnHandlerCalled;
+    }
+
+    @Override
+    public void preAddColumnHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName, HColumnDescriptor column) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preAddColumnHandlerCalled = true;
+    }
+
+    @Override
+    public void postAddColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+        byte[] tableName, HColumnDescriptor column) throws IOException {
+      postAddColumnHandlerCalled = true;
+    }
+
+    public boolean wasAddColumnHandlerCalled() {
+      return preAddColumnHandlerCalled && postAddColumnHandlerCalled;
+    }
+
+    public boolean preAddColumnHandlerCalledOnly() {
+      return preAddColumnHandlerCalled && !postAddColumnHandlerCalled;
+    }
+
+    @Override
+    public void preModifyColumnHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName, HColumnDescriptor descriptor) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preModifyColumnHandlerCalled = true;
+    }
+
+    @Override
+    public void postModifyColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+        byte[] tableName, HColumnDescriptor descriptor) throws IOException {
+      postModifyColumnHandlerCalled = true;
+    }
+
+    public boolean wasModifyColumnHandlerCalled() {
+      return preModifyColumnHandlerCalled && postModifyColumnHandlerCalled;
+    }
+
+    public boolean preModifyColumnHandlerCalledOnly() {
+      return preModifyColumnHandlerCalled && !postModifyColumnHandlerCalled;
+    }
+
+    @Override
+    public void preDeleteColumnHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName, byte[] c) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preDeleteColumnHandlerCalled = true;
+    }
+
+    @Override
+    public void postDeleteColumnHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+        byte[] tableName, byte[] c) throws IOException {
+      postDeleteColumnHandlerCalled = true;
+    }
+
+    public boolean wasDeleteColumnHandlerCalled() {
+      return preDeleteColumnHandlerCalled && postDeleteColumnHandlerCalled;
+    }
+
+    public boolean preDeleteColumnHandlerCalledOnly() {
+      return preDeleteColumnHandlerCalled && !postDeleteColumnHandlerCalled;
+    }
+
+    @Override
+    public void preEnableTableHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preEnableTableHandlerCalled = true;
+    }
+
+    @Override
+    public void postEnableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+        byte[] tableName) throws IOException {
+      postEnableTableHandlerCalled = true;
+    }
+
+    public boolean wasEnableTableHandlerCalled() {
+      return preEnableTableHandlerCalled && postEnableTableHandlerCalled;
+    }
+
+    public boolean preEnableTableHandlerCalledOnly() {
+      return preEnableTableHandlerCalled && !postEnableTableHandlerCalled;
+    }
+
+    @Override
+    public void preDisableTableHandler(ObserverContext<MasterCoprocessorEnvironment> env,
+        byte[] tableName) throws IOException {
+      if (bypass) {
+        env.bypass();
+      }
+      preDisableTableHandlerCalled = true;
+    }
+
+    @Override
+    public void postDisableTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+        byte[] tableName) throws IOException {
+      postDisableTableHandlerCalled = true;
+    }
+
+    public boolean wasDisableTableHandlerCalled() {
+      return preDisableTableHandlerCalled && postDisableTableHandlerCalled;
+    }
+
+    public boolean preDisableTableHandlerCalledOnly() {
+      return preDisableTableHandlerCalled && !postDisableTableHandlerCalled;
+    }
+
+    @Override
     public void preSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,
         final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)
         throws IOException {
@@ -559,6 +783,7 @@
   private static byte[] TEST_CLONE = Bytes.toBytes("observed_clone");
   private static byte[] TEST_FAMILY = Bytes.toBytes("fam1");
   private static byte[] TEST_FAMILY2 = Bytes.toBytes("fam2");
+  private static byte[] TEST_FAMILY3 = Bytes.toBytes("fam3");
 
   @BeforeClass
   public static void setupBeforeClass() throws Exception {
@@ -614,13 +839,20 @@
     admin.createTable(htd);
     // preCreateTable can't bypass default action.
     assertTrue("Test table should be created", cp.wasCreateTableCalled());
+    countDown.await();
+    assertTrue("Table pre create handler called.", cp.wasPreCreateTableHandlerCalled());
+    assertTrue("Table create handler should be called.", cp.wasCreateTableHandlerCalled());
 
+    countDown = new CountDownLatch(1);
     admin.disableTable(TEST_TABLE);
     assertTrue(admin.isTableDisabled(TEST_TABLE));
     // preDisableTable can't bypass default action.
     assertTrue("Coprocessor should have been called on table disable",
       cp.wasDisableTableCalled());
 
+    assertTrue("Disable table handler should be called.", cp.wasDisableTableHandlerCalled());
+    assertTrue("Disable table handler should be called.", cp.wasDisableTableHandlerCalled());
+
     // enable
     assertFalse(cp.wasEnableTableCalled());
     admin.enableTable(TEST_TABLE);
@@ -629,6 +861,8 @@
     assertTrue("Coprocessor should have been called on table enable",
       cp.wasEnableTableCalled());
 
+    assertTrue("Enable table handler should be called.", cp.wasEnableTableHandlerCalled());
+
     admin.disableTable(TEST_TABLE);
     assertTrue(admin.isTableDisabled(TEST_TABLE));
 
@@ -658,8 +892,8 @@
     // preDeleteTable can't bypass default action.
     assertTrue("Coprocessor should have been called on table delete",
       cp.wasDeleteTableCalled());
+    assertTrue("Delete table handler should be called.", cp.wasDeleteTableHandlerCalled());
 
-
     // turn off bypass, run the tests again
     cp.enableBypass(false);
     cp.resetStates();
@@ -667,14 +901,20 @@
     admin.createTable(htd);
     assertTrue("Test table should be created", cp.wasCreateTableCalled());
 
+    countDown.await();
+    assertTrue("Table pre create handler called.", cp.wasPreCreateTableHandlerCalled());
+    assertTrue("Table create handler should be called.", cp.wasCreateTableHandlerCalled());
+
     // disable
     assertFalse(cp.wasDisableTableCalled());
 
+    assertFalse(cp.wasDisableTableHandlerCalled());
     admin.disableTable(TEST_TABLE);
     assertTrue(admin.isTableDisabled(TEST_TABLE));
     assertTrue("Coprocessor should have been called on table disable",
       cp.wasDisableTableCalled());
 
+    assertTrue("Disable table handler should be called.", cp.wasDisableTableHandlerCalled());
     // modify table
     htd.setMaxFileSize(512 * 1024 * 1024);
     modifyTableSync(admin, TEST_TABLE, htd);
@@ -686,6 +926,7 @@
     assertTrue("New column family should have been added to test table",
         cp.wasAddColumnCalled());
 
+    assertTrue("Add column handler should be called.", cp.wasAddColumnHandlerCalled());
     // modify a column family
     HColumnDescriptor hcd = new HColumnDescriptor(TEST_FAMILY2);
     hcd.setMaxVersions(25);
@@ -693,19 +934,24 @@
     assertTrue("Second column family should be modified",
         cp.wasModifyColumnCalled());
 
+    assertTrue("Modify table handler should be called.", cp.wasModifyColumnHandlerCalled());
     // enable
     assertFalse(cp.wasEnableTableCalled());
+    assertFalse(cp.wasEnableTableHandlerCalled());
     admin.enableTable(TEST_TABLE);
     assertTrue(admin.isTableEnabled(TEST_TABLE));
     assertTrue("Coprocessor should have been called on table enable",
         cp.wasEnableTableCalled());
 
+    assertTrue("Enable table handler should be called.", cp.wasEnableTableHandlerCalled());
     // disable again
     admin.disableTable(TEST_TABLE);
     assertTrue(admin.isTableDisabled(TEST_TABLE));
 
     // delete column
     assertFalse("No column family deleted yet", cp.wasDeleteColumnCalled());
+    assertFalse("Delete table column handler should not be called.",
+      cp.wasDeleteColumnHandlerCalled());
     admin.deleteColumn(TEST_TABLE, TEST_FAMILY2);
     HTableDescriptor tableDesc = admin.getTableDescriptor(TEST_TABLE);
     assertNull("'"+Bytes.toString(TEST_FAMILY2)+"' should have been removed",
@@ -713,13 +959,16 @@
     assertTrue("Coprocessor should have been called on column delete",
         cp.wasDeleteColumnCalled());
 
+    assertTrue("Delete table column handler should be called.", cp.wasDeleteColumnHandlerCalled());
     // delete table
     assertFalse("No table deleted yet", cp.wasDeleteTableCalled());
+    assertFalse("Delete table handler should not be called.", cp.wasDeleteTableHandlerCalled());
     admin.deleteTable(TEST_TABLE);
     assertFalse("Test table should have been deleted",
         admin.tableExists(TEST_TABLE));
     assertTrue("Coprocessor should have been called on table delete",
         cp.wasDeleteTableCalled());
+    assertTrue("Delete table handler should be called.", cp.wasDeleteTableHandlerCalled());
   }
 
   private void modifyTableSync(HBaseAdmin admin, byte[] tableName, HTableDescriptor htd)
Index: src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java	(revision 1505907)
+++ src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java	(working copy)
@@ -934,6 +934,8 @@
     Mockito.when(this.serverManager.getDrainingServersList()).thenReturn(destServers);
     // To avoid cast exception in DisableTableHandler process.
     //Server server = new HMaster(HTU.getConfiguration());
+    HTU.getConfiguration().setInt(HConstants.MASTER_PORT, 0);
+    Server server = new HMaster(HTU.getConfiguration());    
     AssignmentManagerWithExtrasForTesting am = setUpMockedAssignmentManager(server,
         this.serverManager);
     AtomicBoolean gate = new AtomicBoolean(false);
@@ -1070,8 +1072,8 @@
     }
 
     @Override
-    public ServerName randomAssignment(List<ServerName> servers) {
-      ServerName randomServerName = super.randomAssignment(servers);
+    public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers) {
+      ServerName randomServerName = super.randomAssignment(region, servers);
       this.gate.set(true);
       return randomServerName;
     }
Index: src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java	(revision 1505907)
+++ src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java	(working copy)
@@ -193,10 +193,10 @@
       assertTimeVaryingMetricCount(1, TABLE_NAME, cf, regionName, "append_");
   
       // One delete where the cf is known
-      assertTimeVaryingMetricCount(1, TABLE_NAME, cf, null, "delete_");
+      assertTimeVaryingMetricCount(1, TABLE_NAME, cf, null, "multidelete_");
   
       // two deletes in the region.
-      assertTimeVaryingMetricCount(2, TABLE_NAME, null, regionName, "delete_");
+      assertTimeVaryingMetricCount(2, TABLE_NAME, null, regionName, "multidelete_");
   
       // Three gets. one for gets. One for append. One for increment.
       assertTimeVaryingMetricCount(4, TABLE_NAME, cf, regionName, "get_");
